# PrometheusRule for Longhorn Storage Alerts
# Phase 4.28: Alert on Longhorn volume degradation — the gap that kube-prometheus-stack
# defaults cannot cover (they handle PVC capacity, not Longhorn replica health)
# Phase 4.28: Added NVMe S.M.A.R.T. alerts (media errors, spare threshold, wear)
#
# Metrics used:
#   longhorn_volume_robustness — per-volume health (0=unknown, 1=healthy, 2=degraded, 3=faulted)
#   smartctl_device_media_errors — unrecoverable NVMe data integrity errors
#   smartctl_device_available_spare — remaining spare NAND blocks (%)
#   smartctl_device_percentage_used — NVMe endurance used (%)
#   smartctl_device_critical_warning — NVMe controller critical warning flags
#   smartctl_device_smart_status — overall SMART health assessment
#   smartctl_device_temperature — current NVMe temperature in Celsius (temperature_type="current")
#
# Note: PVC capacity and node disk are already covered by kube-prometheus-stack defaults:
#   KubePersistentVolumeFillingUp (warning <15%, critical <3%)
#   NodeFilesystemSpaceFillingUp (predictive, 24h/4h window)
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: storage-alerts
  namespace: monitoring
  labels:
    release: prometheus
    app.kubernetes.io/part-of: kube-prometheus-stack
spec:
  groups:
    - name: longhorn
      rules:
        # Longhorn volume has reduced replicas but is still accessible
        - alert: LonghornVolumeDegraded
          expr: longhorn_volume_robustness == 2
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Longhorn volume {{ $labels.pvc }} ({{ $labels.pvc_namespace }}) is degraded"
            description: "Longhorn volume {{ $labels.pvc }} in namespace {{ $labels.pvc_namespace }} has been in degraded state for 10+ minutes. At least one replica is unhealthy. Data is accessible but redundancy is reduced."
            runbook: |
              1. Check volume status in Longhorn UI:
                 https://longhorn.k8s.rommelporras.com

              2. Check replicas (volume label is the Longhorn volume UUID):
                 kubectl-homelab -n longhorn-system get replicas -l longhornvolume={{ $labels.volume }}

              3. Check node health (a node issue may affect replicas):
                 kubectl-homelab get nodes

              4. If a replica is rebuilding, monitor progress in Longhorn UI

        # Longhorn volume has no healthy replicas — data at risk
        - alert: LonghornVolumeReplicaFailed
          expr: longhorn_volume_robustness == 3
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Longhorn volume {{ $labels.pvc }} ({{ $labels.pvc_namespace }}) has no healthy replicas"
            description: "Longhorn volume {{ $labels.pvc }} in namespace {{ $labels.pvc_namespace }} is in faulted state for 5+ minutes. All replicas have failed. Data access is at risk."
            runbook: |
              1. Check volume status immediately in Longhorn UI:
                 https://longhorn.k8s.rommelporras.com

              2. Check which replicas failed:
                 kubectl-homelab -n longhorn-system get replicas -l longhornvolume={{ $labels.volume }} -o wide

              3. Check node conditions:
                 kubectl-homelab get nodes
                 kubectl-homelab describe nodes | grep -A5 Conditions

              4. If nodes are healthy, check Longhorn manager logs:
                 kubectl-homelab -n longhorn-system logs deploy/longhorn-manager --tail=100

              5. Check disk availability on all nodes:
                 kubectl-homelab -n longhorn-system get nodes.longhorn.io -o wide

    - name: nvme-smart
      rules:
        # NVMe drive reported unrecoverable data integrity errors
        # These are physical drive failures — not transient. Any non-zero value is serious.
        - alert: NVMeMediaErrors
          expr: smartctl_device_media_errors > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "NVMe media errors detected on {{ $labels.node }}"
            description: "{{ $labels.device }} on node {{ $labels.node }} has {{ $value }} unrecoverable media error(s). This indicates physical NAND failure. Back up data and plan drive replacement."
            runbook: |
              1. Verify error count in Grafana (NVMe Health row → Longhorn dashboard)

              2. Check full SMART report on the affected node:
                 ssh wawashi@<node>.k8s.rommelporras.com
                 sudo smartctl -a /dev/nvme0

              3. Run extended self-test (non-destructive, ~2 min for NVMe):
                 sudo smartctl -t long /dev/nvme0
                 sudo smartctl -l selftest /dev/nvme0

              4. If errors confirmed, plan drive replacement. Begin Longhorn backup first:
                 https://longhorn.k8s.rommelporras.com

        # NVMe available spare dropped below drive's own threshold
        # SK Hynix drives set this threshold themselves (typically ~10%)
        # alertCTLDeviceAvailableSpareUnderThreshold is the Helm-provided rule;
        # this rule provides a proactive warning before reaching that threshold.
        - alert: NVMeSpareWarning
          expr: smartctl_device_available_spare < 20
          for: 30m
          labels:
            severity: warning
          annotations:
            summary: "NVMe available spare low on {{ $labels.node }}: {{ $value }}%"
            description: "{{ $labels.device }} on node {{ $labels.node }} has only {{ $value }}% spare NAND blocks remaining. Drive endurance is being consumed. Consider planning replacement."
            runbook: |
              1. Check wear level history in Grafana (NVMe Health row → Available Spare panel)

              2. Check full SMART data on the affected node:
                 ssh wawashi@<node>.k8s.rommelporras.com
                 sudo smartctl -a /dev/nvme0

              3. Plan drive replacement before spare reaches 0.
                 SK Hynix HFS512GDE9X081N drives can be sourced as identical replacements.

        # NVMe endurance used above 80% — approaching manufacturer's rated write endurance
        - alert: NVMeWearHigh
          expr: smartctl_device_percentage_used > 80
          for: 30m
          labels:
            severity: warning
          annotations:
            summary: "NVMe wear indicator high on {{ $labels.node }}: {{ $value }}% used"
            description: "{{ $labels.device }} on node {{ $labels.node }} has consumed {{ $value }}% of its rated write endurance. SK Hynix 512GB rated ~300TBW. Plan drive replacement."
            runbook: |
              1. Check Total Bytes Written in Grafana (NVMe Health row → Longhorn dashboard)

              2. Verify SMART data:
                 ssh wawashi@<node>.k8s.rommelporras.com
                 sudo smartctl -a /dev/nvme0

              3. Order replacement drive (SK Hynix HFS512GDE9X081N or equivalent M.2 NVMe).
                 Drain node before replacement:
                 kubectl-homelab drain <node> --ignore-daemonsets --delete-emptydir-data

        # NVMe drive temperature above safe operating threshold
        # SK Hynix HFS512GDE9X081N max operating = 70°C. 65°C = 5°C buffer.
        # Current baseline: ~46°C at idle on all nodes. Only fires under sustained load.
        - alert: NVMeTemperatureHigh
          expr: smartctl_device_temperature{temperature_type="current"} > 65
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "NVMe temperature high on {{ $labels.pod }}: {{ $value }}°C"
            description: "NVMe {{ $labels.device }} on {{ $labels.pod }} has been above 65°C for 10+ minutes (current: {{ $value }}°C). SK Hynix max operating = 70°C. Check airflow and Tdarr transcode load."
            runbook: |
              1. Check temperature trend in Grafana (NVMe Health row → Longhorn Storage dashboard)

              2. Check if Tdarr is running heavy transcode workloads — GPU encoding generates heat:
                 https://tdarr.k8s.rommelporras.com

              3. Verify chassis airflow on the M80q (tiny form factor, relies on fan curve):
                 ssh wawashi@<node>.k8s.rommelporras.com
                 sudo smartctl -a /dev/nvme0  # check all temperature readings

              4. If temperature is approaching 70°C, pause Tdarr immediately and check cooling.
