# v0.2.0 â€” Kubernetes Bootstrap

> **Release:** v0.2.0
> **Phase:** 2
> **Goal:** 3-node HA Kubernetes cluster with Cilium CNI
> **Prerequisite:** [v0.1.0-foundation.md](v0.1.0-foundation.md) completed

---

## Overview

This release bootstraps the Kubernetes cluster:
- kubeadm initialization with HA control plane
- kube-vip for API server VIP (10.10.30.10)
- Cilium CNI for networking and NetworkPolicy

---

## Component Versions

| Component | Version |
|-----------|---------|
| Kubernetes | v1.35.0 |
| containerd | 1.7.x |
| Cilium | 1.18.6 |
| kube-vip | v1.0.3 |

---

## Step 1: Prerequisites (All Nodes)

Run on **all 3 nodes**:

```bash
# Verify cgroup v2 (required for K8s v1.35)
stat -fc %T /sys/fs/cgroup/
# Must show: cgroup2fs

# Disable swap
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
free -h | grep Swap  # Should show 0

# Load kernel modules
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# Verify modules loaded
lsmod | grep -E "overlay|br_netfilter"

# Configure sysctl
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system
sysctl net.ipv4.ip_forward  # Should show 1
```

---

## Step 2: Install containerd (All Nodes)

```bash
# Install containerd
sudo apt install -y containerd

# Generate default config
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml > /dev/null

# Enable systemd cgroup driver (CRITICAL for cgroup v2)
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

# Restart and enable
sudo systemctl restart containerd
sudo systemctl enable containerd

# Verify
sudo systemctl is-active containerd  # Should show: active
grep "SystemdCgroup = true" /etc/containerd/config.toml && echo "SystemdCgroup: OK"
ls -la /run/containerd/containerd.sock  # Socket should exist
```

---

## Step 3: Install Kubernetes Components (All Nodes)

```bash
# Remove any conflicting snap packages
sudo snap remove kubectl kubeadm kubelet 2>/dev/null || true

# Install prerequisites
sudo apt install -y apt-transport-https ca-certificates curl gpg

# Create keyrings directory
sudo mkdir -p -m 755 /etc/apt/keyrings

# Download Kubernetes signing key
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.35/deb/Release.key | \
    sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# Add Kubernetes apt repository
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.35/deb/ /' | \
    sudo tee /etc/apt/sources.list.d/kubernetes.list

# Install components
sudo apt update
sudo apt install -y kubelet kubeadm kubectl

# Hold versions to prevent accidental upgrades
sudo apt-mark hold kubelet kubeadm kubectl

# Enable kubelet (will crashloop until kubeadm init, that's normal)
sudo systemctl enable kubelet

# Verify
kubeadm version
kubectl version --client
kubelet --version
```

---

## Step 4: Install open-iscsi (All Nodes)

Required for Longhorn in the next release:

```bash
sudo apt install -y open-iscsi
sudo systemctl enable --now iscsid
```

---

## Step 5: Set Up kube-vip (k8s-cp1 Only)

Run on **k8s-cp1** only, **before** kubeadm init:

```bash
# Verify network interface (M80q Intel NIC = eno1)
ip -br link | grep -v lo
# Look for interface with state UP

# Set variables
export VIP=10.10.30.10
export INTERFACE=eno1
export KVVERSION=v1.0.3

echo "Using VIP: $VIP on interface: $INTERFACE with kube-vip: $KVVERSION"

# Pull kube-vip image
sudo ctr image pull ghcr.io/kube-vip/kube-vip:${KVVERSION}

# Create manifest directory
sudo mkdir -p /etc/kubernetes/manifests

# Generate kube-vip manifest
sudo ctr run --rm --net-host ghcr.io/kube-vip/kube-vip:${KVVERSION} vip \
    /kube-vip manifest pod \
    --interface $INTERFACE \
    --address $VIP \
    --controlplane \
    --arp \
    --leaderElection | sudo tee /etc/kubernetes/manifests/kube-vip.yaml

# K8s 1.29+ workaround: Use super-admin.conf during bootstrap
sudo sed -i 's|path: /etc/kubernetes/admin.conf|path: /etc/kubernetes/super-admin.conf|' \
    /etc/kubernetes/manifests/kube-vip.yaml

# Verify workaround applied
grep -A1 "hostPath:" /etc/kubernetes/manifests/kube-vip.yaml
# Should show: path: /etc/kubernetes/super-admin.conf
```

---

## Step 6: Initialize Cluster (k8s-cp1)

```bash
# Create kubeadm config
cat <<EOF | sudo tee /etc/kubernetes/kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: "10.10.30.11"
  bindPort: 6443
nodeRegistration:
  name: k8s-cp1
  criSocket: unix:///var/run/containerd/containerd.sock
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
kubernetesVersion: "v1.35.0"
controlPlaneEndpoint: "api.k8s.rommelporras.com:6443"
networking:
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"
apiServer:
  certSANs:
    - "api.k8s.rommelporras.com"
    - "10.10.30.10"
    - "10.10.30.11"
    - "10.10.30.12"
    - "10.10.30.13"
    - "k8s-cp1"
    - "k8s-cp2"
    - "k8s-cp3"
etcd:
  local:
    dataDir: /var/lib/etcd
EOF

# Initialize cluster
sudo kubeadm init --config=/etc/kubernetes/kubeadm-config.yaml --upload-certs
```

**IMPORTANT:** Save the output! It contains:
- Certificate key (for joining control planes)
- Join command (for joining nodes)

```bash
# Revert kube-vip workaround after successful init
sudo sed -i 's|path: /etc/kubernetes/super-admin.conf|path: /etc/kubernetes/admin.conf|' \
    /etc/kubernetes/manifests/kube-vip.yaml

# Configure kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Verify VIP responds
ping -c 3 10.10.30.10
curl -k https://10.10.30.10:6443/healthz
```

---

## Step 7: Install Cilium CNI (k8s-cp1)

```bash
# Install Cilium CLI
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
echo "Installing Cilium CLI: ${CILIUM_CLI_VERSION}"

curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-amd64.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-amd64.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
rm cilium-linux-amd64.tar.gz cilium-linux-amd64.tar.gz.sha256sum

# Verify CLI
cilium version --client

# Install Cilium
cilium install --version 1.18.6

# Wait for Cilium to be ready (2-3 minutes)
cilium status --wait

# Verify Cilium pods
kubectl get pods -n kube-system -l k8s-app=cilium
```

---

## Step 8: Join Additional Control Planes

### On k8s-cp1: Get Join Command

```bash
# Generate new certificate key if needed
sudo kubeadm init phase upload-certs --upload-certs

# Get join command
kubeadm token create --print-join-command
```

### On k8s-cp2 and k8s-cp3: Join Cluster

```bash
# Use the join command with --control-plane and --certificate-key flags
sudo kubeadm join api.k8s.rommelporras.com:6443 \
    --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash> \
    --control-plane \
    --certificate-key <cert-key>

# Configure kubectl on joined nodes
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

---

## Step 9: Copy kubeconfig to Workstation

From your workstation:

```bash
# Create kube directory
mkdir -p ~/.kube

# Copy kubeconfig
scp wawashi@cp1.k8s.rommelporras.com:~/.kube/config ~/.kube/homelab.yaml

# Edit to use VIP instead of node IP
sed -i 's/10.10.30.11/10.10.30.10/' ~/.kube/homelab.yaml

# Add aliases to ~/.zshrc or ~/.bashrc
cat >> ~/.zshrc <<'EOF'
alias kubectl-homelab="kubectl --kubeconfig ~/.kube/homelab.yaml"
alias helm-homelab="helm --kubeconfig ~/.kube/homelab.yaml"
EOF
source ~/.zshrc

# Verify
kubectl-homelab get nodes
```

---

## Verification

```bash
# All nodes Ready
kubectl-homelab get nodes
# NAME      STATUS   ROLES           AGE   VERSION
# k8s-cp1   Ready    control-plane   10m   v1.35.0
# k8s-cp2   Ready    control-plane   5m    v1.35.0
# k8s-cp3   Ready    control-plane   3m    v1.35.0

# All system pods Running
kubectl-homelab get pods -n kube-system

# Cilium status
cilium status

# etcd members (should show 3)
kubectl-homelab -n kube-system exec -it etcd-k8s-cp1 -- etcdctl \
    --endpoints=https://127.0.0.1:2379 \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key \
    member list -w table
```

---

## Checklist

- [ ] containerd installed and SystemdCgroup enabled on all nodes
- [ ] kubeadm, kubelet, kubectl installed on all nodes
- [ ] kube-vip configured on k8s-cp1
- [ ] Cluster initialized with kubeadm init
- [ ] Cilium CNI installed and healthy
- [ ] All 3 control planes joined
- [ ] kubectl-homelab working from workstation
- [ ] All nodes show Ready status
- [ ] etcd shows 3 members

---

## Troubleshooting

### kubeadm init Fails

```bash
# Check containerd
sudo systemctl status containerd

# Check kubelet logs
sudo journalctl -xeu kubelet

# Reset and retry
sudo kubeadm reset -f
sudo rm -rf /etc/kubernetes /var/lib/etcd
```

### Node Won't Join

```bash
# On joining node, check kubelet
sudo journalctl -xeu kubelet

# Verify DNS resolution
nslookup api.k8s.rommelporras.com

# Verify VIP is reachable
ping 10.10.30.10
curl -k https://10.10.30.10:6443/healthz
```

### Cilium Issues

```bash
# Check Cilium logs
kubectl -n kube-system logs -l k8s-app=cilium

# Run connectivity test
cilium connectivity test
```

---

## Next Steps

Proceed to [v0.3.0-storage.md](v0.3.0-storage.md) to install Longhorn distributed storage.
