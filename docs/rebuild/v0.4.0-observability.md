# v0.4.0 — Observability Stack

> **Release:** v0.4.0
> **Phase:** 3.5-3.8
> **Goal:** Gateway API, monitoring, logging, and UPS monitoring
> **Prerequisite:** [v0.3.0-storage.md](v0.3.0-storage.md) completed

---

## Overview

This release installs the complete observability stack:
- **Phase 3.5:** Gateway API with Let's Encrypt TLS via Cilium
- **Phase 3.6:** Prometheus, Grafana, Alertmanager (kube-prometheus-stack)
- **Phase 3.7:** Loki + Alloy for log aggregation
- **Phase 3.8:** NUT for UPS monitoring and graceful shutdown

---

## Component Versions

| Component | Version |
|-----------|---------|
| Gateway API CRDs | v1.4.1 |
| cert-manager | 1.19.2 |
| kube-prometheus-stack | 81.0.0 |
| Loki | 6.49.0 |
| Alloy | 1.5.2 |
| NUT | 2.8.1 |
| nut-exporter | 3.1.1 |

---

## Phase 3.5: Gateway API & HTTPS

### 3.5.1 Install Gateway API CRDs

```bash
# Install standard CRDs (use --server-side to avoid annotation size errors)
kubectl-homelab apply --server-side -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.4.1/standard-install.yaml

# Install experimental TLSRoute CRD (optional)
kubectl-homelab apply --server-side -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.4.1/config/crd/experimental/gateway.networking.k8s.io_tlsroutes.yaml

# Verify CRDs installed
kubectl-homelab get crd | grep gateway
# Should see: gatewayclasses, gateways, httproutes, etc.
```

### 3.5.2 Upgrade Cilium with Gateway API

```bash
# Add Cilium Helm repo if not already added
helm-homelab repo add cilium https://helm.cilium.io/
helm-homelab repo update

# Upgrade Cilium with Gateway API, L2 announcements, kube-proxy replacement
helm-homelab upgrade cilium cilium/cilium \
    --namespace kube-system \
    --version 1.18.6 \
    --values helm/cilium/values.yaml

# Restart Cilium components
kubectl-homelab -n kube-system rollout restart deployment/cilium-operator
kubectl-homelab -n kube-system rollout restart ds/cilium

# Wait for ready
kubectl-homelab -n kube-system rollout status ds/cilium

# Verify GatewayClass exists
kubectl-homelab get gatewayclass
# NAME     CONTROLLER                     ACCEPTED   AGE
# cilium   io.cilium/gateway-controller   True       1m
```

### 3.5.3 Apply Cilium L2 Resources

```bash
# Apply IP Pool for LoadBalancer IPs (10.10.30.20-50)
kubectl-homelab apply -f manifests/cilium/ip-pool.yaml

# Apply L2 Announcement Policy
kubectl-homelab apply -f manifests/cilium/l2-announcement.yaml
```

### 3.5.4 Install cert-manager

```bash
# Create namespace
kubectl-homelab create namespace cert-manager

# Install via OCI (no helm repo add needed)
helm-homelab install cert-manager oci://quay.io/jetstack/charts/cert-manager \
    --namespace cert-manager \
    --version v1.19.2 \
    --set crds.enabled=true \
    --set config.enableGatewayAPI=true

# Wait for ready
kubectl-homelab -n cert-manager rollout status deploy/cert-manager
kubectl-homelab -n cert-manager rollout status deploy/cert-manager-webhook

# Create Cloudflare API token secret
kubectl-homelab create secret generic cloudflare-api-token \
    --namespace cert-manager \
    --from-literal=api-token="$(op read 'op://Kubernetes/Cloudflare DNS API Token/credential')"

# Apply ClusterIssuer (Let's Encrypt + Cloudflare DNS-01)
kubectl-homelab apply -f manifests/cert-manager/cluster-issuer.yaml

# Verify issuers created
kubectl-homelab get clusterissuer
# NAME                  READY   AGE
# letsencrypt-prod      True    1m
# letsencrypt-staging   True    1m
```

### 3.5.5 Create Gateway

```bash
# Apply Gateway resource
kubectl-homelab apply -f manifests/gateway/homelab-gateway.yaml

# Wait for certificate (1-2 minutes for DNS-01 validation)
kubectl-homelab get certificate -A -w
# Wait until READY=True

# Verify Gateway has LoadBalancer IP
kubectl-homelab get gateway -A
# NAME              CLASS    ADDRESS       PROGRAMMED   AGE
# homelab-gateway   cilium   10.10.30.20   True         2m
```

### 3.5.6 Remove kube-proxy

```bash
# Verify services work with Cilium first
kubectl-homelab -n kube-system exec ds/cilium -- cilium-dbg service list | head -20

# Delete kube-proxy (Cilium eBPF handles all service routing)
kubectl-homelab -n kube-system delete ds kube-proxy
kubectl-homelab -n kube-system delete cm kube-proxy

# Clean iptables KUBE-* rules on each node
for node in k8s-cp1 k8s-cp2 k8s-cp3; do
    ssh wawashi@${node}.home.rommelporras.com \
        'sudo iptables-save | grep -v KUBE | sudo iptables-restore'
done

# Verify no KUBE-SVC rules remain
ssh wawashi@k8s-cp1.home.rommelporras.com 'sudo iptables-save | grep KUBE-SVC | wc -l'
# Should be 0
```

### 3.5.7 Verify Gateway API

```bash
# Gateway has IP and is programmed
kubectl-homelab get gateway homelab-gateway

# Certificate is valid
kubectl-homelab get certificate -A

# Test from LAN (should get TLS cert)
curl -v https://10.10.30.20 2>&1 | grep "subject:"
```

---

## Phase 3.6: Monitoring Stack

### 3.6.1 Create Namespace

```bash
kubectl-homelab create namespace monitoring
kubectl-homelab label namespace monitoring pod-security.kubernetes.io/enforce=privileged
# Note: privileged required for node-exporter (hostNetwork, hostPID, hostPath)
```

### 3.6.2 Install kube-prometheus-stack

```bash
# Install via OCI (no helm repo add needed)
helm-homelab install prometheus oci://ghcr.io/prometheus-community/charts/kube-prometheus-stack \
    --namespace monitoring \
    --version 81.0.0 \
    --values helm/prometheus/values.yaml \
    --set grafana.adminPassword="$(op read 'op://Kubernetes/Grafana/password')"

# Wait for all pods (2-3 minutes)
kubectl-homelab -n monitoring get pods -w
```

### 3.6.3 Create Grafana HTTPRoute

```bash
kubectl-homelab apply -f manifests/monitoring/grafana-httproute.yaml

# Verify route
kubectl-homelab get httproute -n monitoring
```

### 3.6.4 Verify Monitoring

```bash
# All monitoring pods running
kubectl-homelab -n monitoring get pods

# Access Grafana
echo "URL: https://grafana.k8s.home.rommelporras.com"
echo "Login: admin"
echo "Password: $(op read 'op://Kubernetes/Grafana/password')"

# Verify in Grafana:
# - Dashboards → Node Exporter / Nodes (should show all 3 nodes)
```

---

## Phase 3.7: Logging Stack

### 3.7.1 Add Grafana Helm Repo

```bash
# Required for Alloy (Loki uses OCI, Alloy doesn't support OCI yet)
helm-homelab repo add grafana https://grafana.github.io/helm-charts
helm-homelab repo update
```

### 3.7.2 Install Loki

```bash
# Install via OCI
helm-homelab install loki oci://ghcr.io/grafana/helm-charts/loki \
    --namespace monitoring \
    --version 6.49.0 \
    --values helm/loki/values.yaml

# Wait for ready
kubectl-homelab -n monitoring rollout status statefulset/loki
```

### 3.7.3 Install Grafana Alloy

```bash
# Install Alloy (log collector, replaces deprecated Promtail)
helm-homelab install alloy grafana/alloy \
    --namespace monitoring \
    --version 1.5.2 \
    --values helm/alloy/values.yaml

# Verify DaemonSet (one pod per node)
kubectl-homelab -n monitoring get ds alloy
# Should show 3/3 ready
```

### 3.7.4 Configure Loki Data Source

```bash
kubectl-homelab apply -f manifests/monitoring/loki-datasource.yaml
```

### 3.7.5 Create ServiceMonitors and Alerts

```bash
kubectl-homelab apply -f manifests/monitoring/loki-servicemonitor.yaml
kubectl-homelab apply -f manifests/monitoring/alloy-servicemonitor.yaml
kubectl-homelab apply -f manifests/monitoring/logging-alerts.yaml
```

### 3.7.6 Verify Logging

```bash
# Loki pod running
kubectl-homelab -n monitoring get pods -l app.kubernetes.io/name=loki

# Alloy DaemonSet running on all nodes
kubectl-homelab -n monitoring get ds alloy

# In Grafana → Explore → Select Loki data source:
# Query: {namespace="monitoring"}
# Query: {source="kubernetes_events"} (K8s events)
```

---

## Phase 3.8: UPS Monitoring

### 3.8.1 Install NUT Server (k8s-cp1)

SSH to k8s-cp1:

```bash
ssh wawashi@k8s-cp1.home.rommelporras.com

# Install NUT
sudo apt install nut nut-server nut-client -y

# Configure UPS driver
sudo tee /etc/nut/ups.conf > /dev/null <<EOF
[cyberpower]
  driver = usbhid-ups
  port = auto
  desc = "CyberPower UPS"
EOF

# Configure NUT mode
echo "MODE=netserver" | sudo tee /etc/nut/nut.conf

# Configure upsd
echo "LISTEN 0.0.0.0 3493" | sudo tee /etc/nut/upsd.conf

# Create udev rules for USB
sudo tee /etc/udev/rules.d/90-nut-ups.rules > /dev/null <<EOF
SUBSYSTEM=="usb", ATTR{idVendor}=="0764", MODE="0664", GROUP="nut"
EOF
sudo udevadm control --reload-rules
sudo udevadm trigger

exit  # Return to workstation
```

From **workstation** (with 1Password access):

```bash
# Configure upsd.users
ssh wawashi@k8s-cp1.home.rommelporras.com "sudo tee /etc/nut/upsd.users > /dev/null" <<EOF
[admin]
  password = $(op read "op://Kubernetes/NUT Admin/password")
  actions = SET
  instcmds = ALL
  upsmon master

[monitor]
  password = $(op read "op://Kubernetes/NUT Monitor/password")
  upsmon slave
EOF

ssh wawashi@k8s-cp1.home.rommelporras.com "sudo chmod 640 /etc/nut/upsd.users && sudo chown root:nut /etc/nut/upsd.users"

# Configure upsmon for cp1 (master)
ssh wawashi@k8s-cp1.home.rommelporras.com "sudo tee /etc/nut/upsmon.conf > /dev/null" <<EOF
MONITOR cyberpower@localhost 1 admin $(op read "op://Kubernetes/NUT Admin/password") master
SHUTDOWNCMD "/sbin/shutdown -h +0"
POWERDOWNFLAG /etc/killpower
MINSUPPLIES 1
POLLFREQ 5
POLLFREQALERT 2
FINALDELAY 5
EOF

ssh wawashi@k8s-cp1.home.rommelporras.com "sudo chmod 640 /etc/nut/upsmon.conf && sudo chown root:nut /etc/nut/upsmon.conf"

# Start NUT services
ssh wawashi@k8s-cp1.home.rommelporras.com "sudo systemctl enable --now nut-server nut-monitor"

# Verify UPS detected
ssh wawashi@k8s-cp1.home.rommelporras.com "upsc cyberpower@localhost"
```

### 3.8.2 Install NUT Clients (k8s-cp2, k8s-cp3)

```bash
# Install nut-client
for node in k8s-cp2 k8s-cp3; do
    ssh wawashi@$node.home.rommelporras.com "sudo apt install nut-client -y"
    ssh wawashi@$node.home.rommelporras.com "echo 'MODE=netclient' | sudo tee /etc/nut/nut.conf"
done
```

Configure **k8s-cp3** (10-minute shutdown timer):

```bash
ssh wawashi@k8s-cp3.home.rommelporras.com "sudo tee /etc/nut/upsmon.conf > /dev/null" <<EOF
MONITOR cyberpower@10.10.30.11 1 monitor $(op read "op://Kubernetes/NUT Monitor/password") slave
MINSUPPLIES 1
POLLFREQ 5
POLLFREQALERT 2
SHUTDOWNCMD "/sbin/shutdown -h +0"
NOTIFYCMD /usr/sbin/upssched
NOTIFYFLAG ONLINE SYSLOG+EXEC
NOTIFYFLAG ONBATT SYSLOG+EXEC
NOTIFYFLAG LOWBATT SYSLOG+EXEC
NOTIFYFLAG FSD SYSLOG+EXEC
EOF

ssh wawashi@k8s-cp3.home.rommelporras.com "sudo tee /etc/nut/upssched.conf > /dev/null" <<EOF
CMDSCRIPT /usr/local/bin/upssched-cmd
PIPEFN /run/nut/upssched.pipe
LOCKFN /run/nut/upssched.lock
AT ONBATT * START-TIMER early-shutdown 600
AT ONLINE * CANCEL-TIMER early-shutdown
AT LOWBATT * EXECUTE forced-shutdown
AT FSD * EXECUTE forced-shutdown
EOF

ssh wawashi@k8s-cp3.home.rommelporras.com "sudo tee /usr/local/bin/upssched-cmd > /dev/null" <<'SCRIPT'
#!/bin/bash
case $1 in
  early-shutdown)
    logger -t upssched "UPS on battery for 10 minutes, initiating early shutdown (cp3)"
    /sbin/shutdown -h +0
    ;;
  forced-shutdown)
    logger -t upssched "UPS low battery or FSD, forcing immediate shutdown"
    /sbin/shutdown -h +0
    ;;
  *)
    logger -t upssched "Unknown command: $1"
    ;;
esac
SCRIPT

ssh wawashi@k8s-cp3.home.rommelporras.com "sudo chmod +x /usr/local/bin/upssched-cmd && sudo chmod 640 /etc/nut/upsmon.conf && sudo chown root:nut /etc/nut/upsmon.conf"
```

Configure **k8s-cp2** (20-minute shutdown timer):

```bash
ssh wawashi@k8s-cp2.home.rommelporras.com "sudo tee /etc/nut/upsmon.conf > /dev/null" <<EOF
MONITOR cyberpower@10.10.30.11 1 monitor $(op read "op://Kubernetes/NUT Monitor/password") slave
MINSUPPLIES 1
POLLFREQ 5
POLLFREQALERT 2
SHUTDOWNCMD "/sbin/shutdown -h +0"
NOTIFYCMD /usr/sbin/upssched
NOTIFYFLAG ONLINE SYSLOG+EXEC
NOTIFYFLAG ONBATT SYSLOG+EXEC
NOTIFYFLAG LOWBATT SYSLOG+EXEC
NOTIFYFLAG FSD SYSLOG+EXEC
EOF

ssh wawashi@k8s-cp2.home.rommelporras.com "sudo tee /etc/nut/upssched.conf > /dev/null" <<EOF
CMDSCRIPT /usr/local/bin/upssched-cmd
PIPEFN /run/nut/upssched.pipe
LOCKFN /run/nut/upssched.lock
AT ONBATT * START-TIMER early-shutdown 1200
AT ONLINE * CANCEL-TIMER early-shutdown
AT LOWBATT * EXECUTE forced-shutdown
AT FSD * EXECUTE forced-shutdown
EOF

ssh wawashi@k8s-cp2.home.rommelporras.com "sudo tee /usr/local/bin/upssched-cmd > /dev/null" <<'SCRIPT'
#!/bin/bash
case $1 in
  early-shutdown)
    logger -t upssched "UPS on battery for 20 minutes, initiating early shutdown (cp2)"
    /sbin/shutdown -h +0
    ;;
  forced-shutdown)
    logger -t upssched "UPS low battery or FSD, forcing immediate shutdown"
    /sbin/shutdown -h +0
    ;;
  *)
    logger -t upssched "Unknown command: $1"
    ;;
esac
SCRIPT

ssh wawashi@k8s-cp2.home.rommelporras.com "sudo chmod +x /usr/local/bin/upssched-cmd && sudo chmod 640 /etc/nut/upsmon.conf && sudo chown root:nut /etc/nut/upsmon.conf"
```

Start nut-monitor on clients:

```bash
for node in k8s-cp2 k8s-cp3; do
    ssh wawashi@$node.home.rommelporras.com "sudo systemctl enable --now nut-monitor"
done
```

### 3.8.3 Configure Kubelet Graceful Shutdown

```bash
for node in k8s-cp1 k8s-cp2 k8s-cp3; do
    ssh wawashi@$node.home.rommelporras.com "sudo tee -a /var/lib/kubelet/config.yaml > /dev/null" <<EOF
shutdownGracePeriod: 120s
shutdownGracePeriodCriticalPods: 30s
EOF
    ssh wawashi@$node.home.rommelporras.com "sudo systemctl restart kubelet"
done

# Verify config
ssh wawashi@k8s-cp1.home.rommelporras.com "sudo grep -A1 shutdown /var/lib/kubelet/config.yaml"
```

### 3.8.4 Deploy NUT Exporter

```bash
# Create secret for NUT credentials
kubectl-homelab create secret generic nut-credentials \
    --namespace monitoring \
    --from-literal=username=monitor \
    --from-literal=password="$(op read 'op://Kubernetes/NUT Monitor/password')"

# Deploy nut-exporter (Deployment, Service, ServiceMonitor)
kubectl-homelab apply -f manifests/monitoring/nut-exporter.yaml

# Deploy UPS alerts
kubectl-homelab apply -f manifests/monitoring/ups-alerts.yaml

# Deploy UPS dashboard (auto-provisioned to Grafana)
kubectl-homelab apply -f manifests/monitoring/ups-dashboard-configmap.yaml
```

### 3.8.5 Verify UPS Monitoring

```bash
# nut-exporter running
kubectl-homelab -n monitoring get pods -l app=nut-exporter

# Test metrics endpoint
kubectl-homelab -n monitoring port-forward svc/nut-exporter 9995:9995 &
curl -s http://localhost:9995/ups_metrics | grep network_ups_tools_battery_charge
kill %1

# All clients can reach server
for node in k8s-cp1 k8s-cp2 k8s-cp3; do
    echo "=== $node ==="
    ssh wawashi@$node.home.rommelporras.com "upsc cyberpower@10.10.30.11 battery.charge 2>/dev/null || upsc cyberpower@localhost battery.charge"
done
```

---

## Final Verification

```bash
echo "=== Cluster Health ==="
kubectl-homelab get nodes
kubectl-homelab get pods -A | grep -v Running | grep -v Completed

echo ""
echo "=== Gateway API ==="
kubectl-homelab get gateway -A
kubectl-homelab get certificate -A

echo ""
echo "=== Monitoring Stack ==="
kubectl-homelab -n monitoring get pods | head -15

echo ""
echo "=== Logging Stack ==="
kubectl-homelab -n monitoring get pods -l app.kubernetes.io/name=loki
kubectl-homelab -n monitoring get ds alloy

echo ""
echo "=== UPS Status ==="
ssh wawashi@k8s-cp1.home.rommelporras.com "upsc cyberpower@localhost ups.status"
ssh wawashi@k8s-cp1.home.rommelporras.com "upsc cyberpower@localhost battery.charge"

echo ""
echo "=== Access URLs ==="
echo "Grafana:       https://grafana.k8s.home.rommelporras.com"
echo "UPS Dashboard: https://grafana.k8s.home.rommelporras.com/d/ups-monitoring"
```

---

## Checklist

### Phase 3.5: Gateway API
- [ ] Gateway API CRDs installed (v1.4.1)
- [ ] Cilium upgraded with Gateway API enabled
- [ ] L2 announcement policy applied
- [ ] cert-manager installed with Cloudflare DNS-01
- [ ] Gateway has LoadBalancer IP (10.10.30.20)
- [ ] Wildcard TLS certificate issued
- [ ] kube-proxy removed

### Phase 3.6: Monitoring
- [ ] monitoring namespace created with privileged PSS
- [ ] kube-prometheus-stack installed
- [ ] Grafana HTTPRoute created
- [ ] Grafana accessible via HTTPS
- [ ] Node metrics visible in dashboards

### Phase 3.7: Logging
- [ ] Loki installed (SingleBinary mode)
- [ ] Alloy DaemonSet running (3/3)
- [ ] Loki data source configured in Grafana
- [ ] ServiceMonitors and alerts created
- [ ] Logs visible in Grafana Explore

### Phase 3.8: UPS Monitoring
- [ ] NUT server running on k8s-cp1
- [ ] NUT clients running on k8s-cp2, k8s-cp3
- [ ] Staggered shutdown timers configured (10m/20m)
- [ ] Kubelet graceful shutdown configured
- [ ] nut-exporter deployed
- [ ] UPS dashboard visible in Grafana
- [ ] UPS alerts configured

---

## Staggered Shutdown Reference

| Node | Trigger | Timer |
|------|---------|-------|
| k8s-cp3 | ONBATT | 10 minutes |
| k8s-cp2 | ONBATT | 20 minutes |
| k8s-cp1 | Low Battery (LB) | Native NUT |

With ~70 minute UPS runtime at 9% load, these timers provide ample safety margin.

---

## Troubleshooting

### Certificate Not Issued

```bash
# Check cert-manager logs
kubectl-homelab -n cert-manager logs -l app=cert-manager

# Check certificate status
kubectl-homelab describe certificate wildcard-k8s-home-tls

# Check challenges
kubectl-homelab get challenges -A
```

### Grafana Not Accessible

```bash
# Check HTTPRoute
kubectl-homelab describe httproute grafana -n monitoring

# Check Gateway
kubectl-homelab describe gateway homelab-gateway

# Test directly via port-forward
kubectl-homelab -n monitoring port-forward svc/prometheus-grafana 3000:80
```

### NUT Connection Issues

```bash
# Check NUT server status
ssh wawashi@k8s-cp1.home.rommelporras.com "sudo systemctl status nut-server"

# Check USB connection
ssh wawashi@k8s-cp1.home.rommelporras.com "lsusb | grep -i cyber"

# Check NUT driver
ssh wawashi@k8s-cp1.home.rommelporras.com "sudo upsdrvctl start"
```

---

## Congratulations!

Your homelab cluster is now fully operational with:
- 3-node HA Kubernetes cluster
- Distributed storage (Longhorn)
- Gateway API with automatic TLS
- Full monitoring stack (Prometheus/Grafana)
- Centralized logging (Loki/Alloy)
- UPS monitoring with graceful shutdown

Return to [README.md](README.md) for the complete rebuild documentation index.
