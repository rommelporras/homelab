# v0.27.0 — Alerting & Observability Improvements

> **Release:** v0.27.0
> **Phase:** 4.28 (Alerting & Observability Improvements)
> **Goal:** Close all alerting gaps discovered in Feb 2026 audit, add 12 Blackbox probes, NVMe S.M.A.R.T. monitoring, Longhorn and cert-manager ServiceMonitors, 20 new PrometheusRules, ARR stack exporters, ARR stall resolver CronJob, and 11 Grafana dashboards
> **Prerequisite:** [v0.26.0-version-automation.md](v0.26.0-version-automation.md) completed, all services running

---

## Overview

This release closes three categories of monitoring gaps discovered during the Feb 2026 audit:

```
Alerting & Observability Improvements
│
├── Bug Fixes (3 existing alerts broken)
│   ├── JellyfinDown — alert existed but no probe, would never fire
│   ├── AdGuardDNSUnreachable — wrong labels, not discovered by Prometheus Operator
│   └── UptimeKumaDown — probe existed but no PrometheusRule acted on it
│
├── New Blackbox Probes (11 services total)
│   ├── Public services: Ghost, Invoicetron, Portfolio
│   ├── ARR services: Jellyfin (fix), Seerr, Tdarr, Byparr
│   └── Existing: Ollama, Karakeep, AdGuard, Uptime Kuma (unchanged)
│
├── New PrometheusRules (20 new alerts)
│   ├── Service health: GhostDown, InvoicetronDown, PortfolioDown
│   ├── ARR services: SeerrDown, TdarrDown, ByparrDown, UptimeKumaDown, ArrQueueWarning, ArrQueueError
│   ├── Infrastructure: LonghornVolumeDegraded, LonghornVolumeReplicaFailed
│   ├── NVMe S.M.A.R.T.: NVMeMediaErrors, NVMeSpareWarning, NVMeWearHigh
│   ├── Certificates: CertificateExpiringSoon, CertificateExpiryCritical, CertificateNotReady
│   ├── Cloudflare: CloudflareTunnelDegraded, CloudflareTunnelDown
│   └── API server: KubeApiserverFrequentRestarts
│
├── New ServiceMonitors (4 new)
│   ├── Longhorn volume health (longhorn-system, 60s)
│   ├── cert-manager certificate expiry (cert-manager, 300s)
│   ├── tdarr-exporter ARR metrics (arr-stack, 60s)
│   └── qbittorrent-exporter ARR metrics (arr-stack, 30s)
│
├── New Exporters (3 new)
│   ├── smartctl-exporter DaemonSet (NVMe S.M.A.R.T. on all 3 nodes, via Helm)
│   ├── tdarr-exporter Deployment (Tdarr stats API → Prometheus)
│   └── qbittorrent-exporter Deployment (qBittorrent WebUI API → Prometheus)
│
├── Cleanup
│   └── LokiStorageLow alert removed (redundant with default KubePersistentVolumeFillingUp)
│
├── ARR Stall Resolver (CronJob every 30 min)
│   └── Auto-resolves stalled downloads: blocklist dead release, switch to Any quality, trigger re-search
│
├── Additional Infrastructure
│   ├── Prometheus + Alertmanager HTTPRoutes (exposed at *.k8s.rommelporras.com)
│   ├── Homepage Widgets (Prometheus targets count + Alertmanager firing count)
│   └── kubeadm Bind Addresses (etcd/scheduler/controller-manager → 0.0.0.0 for scraping)
│
└── Grafana Dashboards (11 total updated/created)
    ├── NEW: Service Health (12-service UP/DOWN grid)
    ├── NEW: Longhorn Storage (NVMe S.M.A.R.T. panels)
    ├── REWRITTEN: kube-vip (removed phantom panel, fixed metrics)
    ├── OVERHAULED: Network (per-node queries, cp1/cp2/cp3 color coding)
    ├── EXPANDED: ARR Stack (8 rows: Tdarr stats, qBit activity, Recent Activity)
    └── UPDATED: Scraparr, Jellyfin, Tailscale, Claude, UPS, version-checker
```

---

## Prerequisites: Fix kubeadm Component Bind Addresses

By default, etcd, kube-scheduler, and kube-controller-manager only listen on `127.0.0.1`. Without this fix, Prometheus scrapes fail silently for these components. Apply on all 3 nodes before any Phase 4.28 work:

```bash
for node in cp1.k8s.rommelporras.com cp2.k8s.rommelporras.com cp3.k8s.rommelporras.com; do
  echo "=== $node ==="
  ssh wawashi@$node 'sudo sed -i "s/--bind-address=127.0.0.1/--bind-address=0.0.0.0/" /etc/kubernetes/manifests/kube-scheduler.yaml'
  ssh wawashi@$node 'sudo sed -i "s/--bind-address=127.0.0.1/--bind-address=0.0.0.0/" /etc/kubernetes/manifests/kube-controller-manager.yaml'
  ssh wawashi@$node 'sudo sed -i "s|listen-metrics-urls=http://127.0.0.1:2381|listen-metrics-urls=http://0.0.0.0:2381|" /etc/kubernetes/manifests/etcd.yaml'
done
# Kubelet detects manifest changes and restarts static pods within ~5-10s
```

Verify after 30s:

```bash
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-stack-prometheus 9090:9090 &
sleep 5
curl -s 'http://localhost:9090/api/v1/targets?state=active' | \
  python3 -c "
import sys,json
d=json.load(sys.stdin)
jobs=[t['labels']['job'] for t in d['data']['activeTargets']]
for j in ['kube-etcd','kube-scheduler','kube-controller-manager']:
    print(j, '✓' if j in jobs else '✗ MISSING')
"
kill $(lsof -ti:9090)
# Expected: all 3 show ✓
```

---

## Step 1: Fix Broken Alert — AdGuard DNS Labels

The existing `adguard-dns-alert.yaml` had wrong Prometheus Operator discovery labels:

```bash
# Apply the fixed file (labels corrected to release: prometheus)
kubectl-homelab apply -f manifests/monitoring/alerts/adguard-dns-alert.yaml

# Verify it appears in Prometheus rules
kubectl-homelab -n monitoring exec prometheus-prometheus-kube-prometheus-stack-prometheus-0 -- \
  wget -qO- http://localhost:9090/api/v1/rules | \
  python3 -c "import sys,json; rules=[r for g in json.load(sys.stdin)['data']['groups'] for r in g['rules']]; print(next(r['name'] for r in rules if r['name']=='AdGuardDNSUnreachable'))"
# Expected: AdGuardDNSUnreachable
```

---

## Step 2: Fix Broken Alert — Create Jellyfin Blackbox Probe

The `JellyfinDown` alert existed but had no matching Blackbox Probe. Create the probe:

```bash
kubectl-homelab apply -f manifests/monitoring/probes/jellyfin-probe.yaml

# Verify metric appears (may take up to 60s)
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-stack-prometheus 9090:9090 &
sleep 5
curl -s 'http://localhost:9090/api/v1/query?query=probe_success{job="jellyfin"}' | \
  python3 -c "import sys,json; d=json.load(sys.stdin); print('OK' if d['data']['result'] else 'MISSING')"
kill $(lsof -ti:9090)
```

---

## Step 3: Fix Broken Alert — Create Uptime Kuma PrometheusRule

The Uptime Kuma probe existed but no alert rule acted on its `probe_success` metric:

```bash
kubectl-homelab apply -f manifests/monitoring/alerts/uptime-kuma-alerts.yaml
```

---

## Step 4: Add Public Service Probes & Alerts

Ghost, Invoicetron, and Portfolio are public-facing via Cloudflare Tunnel with zero monitoring:

```bash
# Probes
kubectl-homelab apply \
  -f manifests/monitoring/probes/ghost-probe.yaml \
  -f manifests/monitoring/probes/invoicetron-probe.yaml \
  -f manifests/monitoring/probes/portfolio-probe.yaml

# Alert rules
kubectl-homelab apply \
  -f manifests/monitoring/alerts/ghost-alerts.yaml \
  -f manifests/monitoring/alerts/invoicetron-alerts.yaml \
  -f manifests/monitoring/alerts/portfolio-alerts.yaml

# Verify metrics visible (wait 60s for first scrape)
# Check Grafana → Service Health dashboard → Ghost/Invoicetron/Portfolio should show UP
```

---

## Step 5: Add ARR Service Probes & Alerts

Seerr, Tdarr, and Byparr were user-facing ARR services with zero monitoring:

```bash
# Probes
kubectl-homelab apply \
  -f manifests/monitoring/probes/seerr-probe.yaml \
  -f manifests/monitoring/probes/tdarr-probe.yaml \
  -f manifests/monitoring/probes/byparr-probe.yaml

# Alert rules (SeerrDown, TdarrDown, ByparrDown added to arr-alerts.yaml)
kubectl-homelab apply -f manifests/monitoring/alerts/arr-alerts.yaml
```

---

## Step 6: Add Longhorn Volume Metrics & Alerts

Longhorn has no default ServiceMonitor — metrics were not scraped at all:

```bash
# ServiceMonitor (scrapes longhorn-system:9500 at 60s)
kubectl-homelab apply -f manifests/monitoring/servicemonitors/longhorn-servicemonitor.yaml

# Wait for first scrape (up to 60s), then verify
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-stack-prometheus 9090:9090 &
sleep 65
curl -s 'http://localhost:9090/api/v1/query?query=longhorn_volume_robustness' | \
  python3 -c "import sys,json; d=json.load(sys.stdin); print(f'{len(d[\"data\"][\"result\"])} volumes scraped')"
# Expected: 31+ volumes scraped
kill $(lsof -ti:9090)

# Alert rules (LonghornVolumeDegraded warning, LonghornVolumeReplicaFailed critical,
# NVMeMediaErrors critical, NVMeSpareWarning warning, NVMeWearHigh warning)
kubectl-homelab apply -f manifests/monitoring/alerts/storage-alerts.yaml
```

**Note on `longhorn_volume_robustness` values:** 0=unknown (detached/unmounted, expected), 1=healthy, 2=degraded (alert fires), 3=faulted (critical alert fires). The `invoicetron-backups` PVC normally shows 0 (detached) — this is expected and does NOT trigger an alert.

---

## Step 7: Add cert-manager Certificate Metrics & Alerts

cert-manager has no default ServiceMonitor — TLS certificate expiry was silent:

```bash
# ServiceMonitor (scrapes cert-manager:9402 at 300s)
kubectl-homelab apply -f manifests/monitoring/servicemonitors/certmanager-servicemonitor.yaml

# Wait for first scrape (up to 5 minutes), then verify
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-stack-prometheus 9090:9090 &
sleep 310
curl -s 'http://localhost:9090/api/v1/query?query=certmanager_certificate_expiration_timestamp_seconds' | \
  python3 -c "import sys,json; d=json.load(sys.stdin); print(f'{len(d[\"data\"][\"result\"])} certificates scraped')"
# Expected: 4 certificates (3 wildcard + inteldeviceplugins-serving-cert)
kill $(lsof -ti:9090)

# Alert rules (CertificateExpiringSoon 30d warning, CertificateExpiryCritical 7d critical, CertificateNotReady critical)
kubectl-homelab apply -f manifests/monitoring/alerts/cert-alerts.yaml
```

---

## Step 8: Add Cloudflare Tunnel Alerts

The CloudFlare ServiceMonitor already existed (job=`cloudflared`). Only alert rules were missing:

```bash
kubectl-homelab apply -f manifests/monitoring/alerts/cloudflare-alerts.yaml
```

---

## Step 9: Add API Server Restart Alert

Discovered Feb 19, 2026: `kube-apiserver-k8s-cp3` had 30 restarts in 34 days (~1/day). Each restart drops the kube-vip VIP for ~2 minutes. `KubeAPIDown` only fires on full outage — this alert catches frequent-brief restart patterns:

```bash
kubectl-homelab apply -f manifests/monitoring/alerts/apiserver-alerts.yaml

# Verify current state (should NOT be firing — threshold is >5 restarts/24h)
# Normal background rate: cp1=1-2, cp2=0-1, cp3=1-2 per 24h
```

---

## Step 10: Remove Redundant LokiStorageLow Alert

`LokiStorageLow` was redundant with the default `KubePersistentVolumeFillingUp`:

```bash
kubectl-homelab apply -f manifests/monitoring/alerts/logging-alerts.yaml

# Verify LokiStorageLow is gone from Prometheus rules
# Verify KubePersistentVolumeFillingUp still covers Loki PVC (it does — covers ALL PVCs)
```

---

## Step 11: Install smartctl-exporter (NVMe S.M.A.R.T. DaemonSet)

Adds NVMe health monitoring (temperature, wear, spare capacity, TBW, media errors) on all 3 nodes. **Critical:** pin to `/dev/nvme0` (not `/dev/nvme0n1` or auto-scan — auto-scan picks up Longhorn iSCSI virtual devices and produces bogus data).

```bash
helm-homelab repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm-homelab repo update

helm-homelab upgrade --install smartctl-exporter \
  prometheus-community/prometheus-smartctl-exporter \
  --version 0.16.0 \
  -n monitoring \
  -f helm/smartctl-exporter/values.yaml

# Verify DaemonSet (should be 3/3 on cp1, cp2, cp3)
kubectl-homelab -n monitoring get daemonset smartctl-exporter
kubectl-homelab -n monitoring get pods -l app.kubernetes.io/name=prometheus-smartctl-exporter

# Verify metrics (wait 60s for first scrape)
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-stack-prometheus 9090:9090 &
sleep 65
curl -s 'http://localhost:9090/api/v1/query?query=smartctl_device_smart_status' | \
  python3 -c "import sys,json; d=json.load(sys.stdin); print(f'{len(d[\"data\"][\"result\"])} drives reporting')"
# Expected: 3 drives reporting (one per node)
kill $(lsof -ti:9090)
```

**Metrics available:**
- `smartctl_device_smart_status` — 1=PASS, 0=FAIL overall health
- `smartctl_device_temperature` — drive temperature (°C)
- `smartctl_device_available_spare` — % spare capacity remaining
- `smartctl_device_percentage_used` — wear level (% of rated lifetime used)
- `smartctl_device_data_units_written_total` — total bytes written (TBW)
- `smartctl_device_power_on_hours_total` — power-on hours

**Drives on all 3 nodes:** SK Hynix HFS512GDE9X081N 512GB (`/dev/nvme0`), firmware 41730C20

---

## Step 12: Deploy tdarr-exporter

Exposes Tdarr library stats (file counts, transcode queue, health check status, space saved, errors) as Prometheus metrics:

```bash
kubectl-homelab apply -f manifests/arr-stack/tdarr/tdarr-exporter.yaml
kubectl-homelab apply -f manifests/monitoring/servicemonitors/tdarr-servicemonitor.yaml

# Verify pod running
kubectl-homelab -n arr-stack get pods -l app=tdarr-exporter

# Verify metrics reachable
kubectl-homelab -n arr-stack port-forward svc/tdarr-exporter 9090:9090 &
sleep 5
curl -s http://localhost:9090/metrics | grep "tdarr_" | head -10
kill $(lsof -ti:9090)
```

---

## Step 13: Deploy qbittorrent-exporter

Exposes qBittorrent download stats (active torrents, speeds, ratio, session totals) as Prometheus metrics. Requires the qBittorrent password from 1Password:

```bash
# Create the required secret (imperative — never commit)
eval $(op signin)
kubectl-homelab -n arr-stack create secret generic qbittorrent-exporter-secret \
  --from-literal=QBITTORRENT_PASS="$(op read 'op://Kubernetes/ARR Stack/password')"

# Deploy exporter
kubectl-homelab apply -f manifests/arr-stack/qbittorrent/qbittorrent-exporter.yaml
kubectl-homelab apply -f manifests/monitoring/servicemonitors/qbittorrent-servicemonitor.yaml

# Verify pod running
kubectl-homelab -n arr-stack get pods -l app=qbittorrent-exporter

# Verify metrics reachable
kubectl-homelab -n arr-stack port-forward svc/qbittorrent-exporter 8000:8000 &
sleep 5
curl -s http://localhost:8000/metrics | grep "qbittorrent_" | head -10
kill $(lsof -ti:8000)
```

**Secret:** `op://Kubernetes/ARR Stack/password` (the qBittorrent web UI password, same as for manual login)

---

## Step 14: Deploy ARR Stall Resolver

Automatically resolves stalled/failed Sonarr and Radarr downloads every 30 minutes: switches quality profile to "Any", blocklists the dead release, triggers re-search. Uses the existing `arr-api-keys` Secret. `ArrQueueWarning` (warning, 60m) and `ArrQueueError` (critical, 15m) alerts were added to `arr-alerts.yaml` alongside this — two resolver cycles run before the warning fires.

```bash
kubectl-homelab apply -f manifests/arr-stack/stall-resolver/configmap.yaml
kubectl-homelab apply -f manifests/arr-stack/stall-resolver/cronjob.yaml

# Verify CronJob created
kubectl-homelab -n arr-stack get cronjob arr-stall-resolver
# Expected: SCHEDULE=*/30 * * * *  SUSPEND=False

# Trigger a manual test run
kubectl-homelab -n arr-stack create job --from=cronjob/arr-stall-resolver arr-stall-resolver-test
sleep 15
kubectl-homelab -n arr-stack logs -l job-name=arr-stall-resolver-test
# Expected: script runs, logs each queue check, exits 0
kubectl-homelab -n arr-stack delete job arr-stall-resolver-test
```

---

## Step 15: Expose Prometheus + Alertmanager via HTTPRoute

Required for Homepage widgets and for direct troubleshooting access to Prometheus and Alertmanager UIs:

```bash
kubectl-homelab apply -f manifests/monitoring/grafana/prometheus-httproute.yaml
kubectl-homelab apply -f manifests/monitoring/grafana/alertmanager-httproute.yaml

# Verify accessible
curl -sI https://prometheus.k8s.rommelporras.com | head -1
curl -sI https://alertmanager.k8s.rommelporras.com | head -1
# Expected: HTTP/2 200
```

---

## Step 16: Update Homepage Widgets

Adds Prometheus targets count and Alertmanager firing count (Watchdog excluded) to the Homepage dashboard:

```bash
kubectl-homelab apply -k manifests/home/homepage/

# Verify pods restarted with new config
kubectl-homelab -n home rollout status deployment homepage
# Expected: deployment "homepage" successfully rolled out

# Open https://portal.k8s.rommelporras.com
# Verify: Prometheus widget shows active targets count
# Verify: Alertmanager widget shows firing alerts (should be 1: Watchdog only)
```

---

## Step 17: Enable Grafana Homelab Folder + Apply Dashboards

Phase 4.28 enabled Grafana's `folderAnnotation` feature so all dashboards appear in a "Homelab" folder instead of the root. This requires a Helm values update to kube-prometheus-stack:

```bash
# Apply updated Helm values (enables sidecar.dashboards.folderAnnotation)
helm-homelab upgrade prometheus \
  oci://ghcr.io/prometheus-community/charts/kube-prometheus-stack \
  --version 81.0.0 \
  -n monitoring \
  -f helm/prometheus/values.yaml \
  --set grafana.adminPassword="$(op read 'op://Kubernetes/Grafana/password')" \
  --reuse-values
```

**Note on Grafana RWO PVC restart:** If Helm upgrade triggers a rolling update, the new pod cannot attach the RWO PVC while the old pod holds it. Fix:
1. Find old ReplicaSet: `kubectl-homelab get rs -n monitoring -l app.kubernetes.io/name=grafana`
2. Scale it down: `kubectl-homelab scale replicaset -n monitoring <old-rs-name> --replicas=0`
3. The new pod will then attach the volume and start normally.

Apply all Grafana dashboards:

```bash
kubectl-homelab apply -f manifests/monitoring/dashboards/service-health-dashboard-configmap.yaml
kubectl-homelab apply -f manifests/monitoring/dashboards/longhorn-dashboard-configmap.yaml
kubectl-homelab apply -f manifests/monitoring/dashboards/kube-vip-dashboard-configmap.yaml
kubectl-homelab apply -f manifests/monitoring/dashboards/network-dashboard-configmap.yaml
kubectl-homelab apply -f manifests/monitoring/dashboards/arr-stack-dashboard-configmap.yaml
kubectl-homelab apply -f manifests/monitoring/dashboards/scraparr-dashboard-configmap.yaml
kubectl-homelab apply -f manifests/monitoring/dashboards/jellyfin-dashboard-configmap.yaml
kubectl-homelab apply -f manifests/monitoring/dashboards/tailscale-dashboard-configmap.yaml
kubectl-homelab apply -f manifests/monitoring/dashboards/claude-dashboard-configmap.yaml
kubectl-homelab apply -f manifests/monitoring/dashboards/ups-dashboard-configmap.yaml
kubectl-homelab apply -f manifests/monitoring/dashboards/version-checker-dashboard-configmap.yaml

# Grafana sidecar picks up changes within ~30s
# Hard-refresh Grafana: https://grafana.k8s.rommelporras.com
# All 11 dashboards should appear in "Homelab" folder
```

---

## Step 18: Pre-Release Alert Gaps + Tdarr/qBit Operational Alerts

Dashboard audit cross-referenced all 11 Grafana dashboards against existing PrometheusRule files. Fixes one bug, adds 10 new alert rules, one new probe, and one new PrometheusRule file.

### 18a: Fix ClaudeCodeNoActivity Timezone Bug

```bash
# Already fixed in claude-alerts.yaml (hour>=17 → hour>=9)
kubectl-homelab apply -f manifests/monitoring/alerts/claude-alerts.yaml

# Verify: alert should NOT fire during business hours Manila (09:00-18:00 PHT = 01:00-10:00 UTC)
# Correct window: hour() >= 9 and hour() <= 11 UTC = 5-7pm Manila
```

### 18b: Add JellyfinHighMemory, BazarrDown, Tdarr + qBit Alerts

```bash
kubectl-homelab apply -f manifests/monitoring/alerts/arr-alerts.yaml

# Verify all new rules load with health: ok
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-prometheus 9090:9090 &
sleep 3
curl -s 'http://localhost:9090/api/v1/rules' | python3 -c "
import sys, json
data = json.load(sys.stdin)
targets = {'JellyfinHighMemory','BazarrDown','TdarrTranscodeErrors','TdarrTranscodeErrorsBurst',
           'TdarrHealthCheckErrors','TdarrHealthCheckErrorsBurst','QBittorrentStalledDownloads'}
found = {r['name']: r['health'] for g in data['data']['groups']
         for r in g['rules'] if r.get('name') in targets}
for name, health in sorted(found.items()):
    print(f'  {health:8s} {name}')
print(f'Missing: {targets - set(found)}' if targets - set(found) else 'All found.')
"
kill \$(lsof -ti:9090)
# Expected: all 7 rules show health: ok, none firing
```

**Alert routing:**
- Warning → Discord #status (auto-resolved when condition clears)
- Critical (`TdarrTranscodeErrorsBurst`, `TdarrHealthCheckErrorsBurst`) → Discord #incidents + email

**Tdarr counter design:** Uses `increase(metric[1h])` not raw value — Tdarr error counters are cumulative (73+ historical health check errors exist). Raw `> 0` would fire permanently. `increase()[1h]` measures new errors; auto-resolves ~1h after errors stop.

### 18c: Add NVMeTemperatureHigh

```bash
kubectl-homelab apply -f manifests/monitoring/alerts/storage-alerts.yaml

# Verify metric name (confirmed as smartctl_device_temperature, not _celsius):
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-prometheus 9090:9090 &
sleep 3
curl -s 'http://localhost:9090/api/v1/query?query=smartctl_device_temperature%7Btemperature_type%3D%22current%22%7D' | \
  python3 -c "import sys,json; [print(r['metric']['pod'],'=',r['value'][1],'°C') for r in json.load(sys.stdin)['data']['result']]"
kill \$(lsof -ti:9090)
# Expected: 3 pods, each showing ~46°C at idle (alert threshold: 65°C)
```

### 18d: Create ServiceHighResponseTime Alert

```bash
kubectl-homelab apply -f manifests/monitoring/alerts/service-health-alerts.yaml

# Verify no false positives (all probes should be well under 5s)
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-prometheus 9090:9090 &
sleep 3
curl -s 'http://localhost:9090/api/v1/query?query=max(probe_duration_seconds%7Bjob%3D~%22ghost%7Cinvoicetron%7Cportfolio%7Cjellyfin%7Cseerr%7Ckarakeep%22%7D)by(job)' | \
  python3 -c "import sys,json; [print(f'  {r[\"metric\"][\"job\"]}: {float(r[\"value\"][1]):.3f}s') for r in json.load(sys.stdin)['data']['result']]"
kill \$(lsof -ti:9090)
# Expected: all under 1s (alert fires at >5s for 5m)
```

### 18e: Add Bazarr Probe + Dashboard Panel

```bash
kubectl-homelab apply -f manifests/monitoring/probes/bazarr-probe.yaml

# Wait 60s for first scrape, then verify
sleep 60
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-prometheus 9090:9090 &
sleep 3
curl -s 'http://localhost:9090/api/v1/query?query=probe_success%7Bjob%3D%22bazarr%22%7D' | \
  python3 -c "import sys,json; d=json.load(sys.stdin); print('UP' if d['data']['result'] and d['data']['result'][0]['value'][1]=='1' else 'DOWN or missing')"
kill \$(lsof -ti:9090)
# Expected: UP

# Apply updated Service Health dashboard (adds Bazarr stat panel)
kubectl-homelab apply -f manifests/monitoring/dashboards/service-health-dashboard-configmap.yaml

# Hard-refresh Grafana → Service Health dashboard
# Expected: Bazarr panel appears in row 3 (alongside Karakeep, Ollama, AdGuard DNS)
```

---

## Verification Checklist

```bash
# 1. All probes returning UP
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-stack-prometheus 9090:9090 &
sleep 3
# Should return 12 results (1 per service), all value=1
curl -s 'http://localhost:9090/api/v1/query?query=probe_success==1' | \
  python3 -c "import sys,json; d=json.load(sys.stdin); print(f'{len(d[\"data\"][\"result\"])} probes UP')"
kill $(lsof -ti:9090)

# 2. All 3 Phase 4.28 PrometheusRules exist
kubectl-homelab get prometheusrules -n monitoring | grep -E "uptime-kuma|ghost|invoicetron|portfolio|storage|cert|cloudflare|apiserver"
kubectl-homelab get prometheusrule arr-alerts -n monitoring  # SeerrDown/TdarrDown/ByparrDown

# 3. Longhorn metrics (all volumes healthy)
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-stack-prometheus 9090:9090 &
sleep 3
curl -s 'http://localhost:9090/api/v1/query?query=longhorn_volume_robustness' | \
  python3 -c "import sys,json; d=json.load(sys.stdin); print(f'{len(d[\"data\"][\"result\"])} volumes')"
kill $(lsof -ti:9090)

# 4. cert-manager certificates (all >30 days from expiry)
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-stack-prometheus 9090:9090 &
sleep 3
curl -s 'http://localhost:9090/api/v1/query?query=certmanager_certificate_expiration_timestamp_seconds' | \
  python3 -c "
import sys,json,time
d=json.load(sys.stdin)
for r in d['data']['result']:
    ns=r['metric']['namespace']; name=r['metric']['name']
    days=(float(r['value'][1])-time.time())/86400
    print(f'{ns}/{name}: {days:.0f}d remaining')
"

# 5. smartctl-exporter DaemonSet running on all 3 nodes
kubectl-homelab -n monitoring get pods -l app.kubernetes.io/name=prometheus-smartctl-exporter -o wide
# Expected: 3 pods, one each on k8s-cp1, k8s-cp2, k8s-cp3

# 6. NVMe S.M.A.R.T. status (all drives PASS)
kubectl-homelab -n monitoring port-forward svc/prometheus-kube-prometheus-stack-prometheus 9090:9090 &
sleep 3
curl -s 'http://localhost:9090/api/v1/query?query=smartctl_device_smart_status' | \
  python3 -c "import sys,json; d=json.load(sys.stdin); [print(r['metric'].get('node','?'), '=', 'PASS' if r['value'][1]=='1' else 'FAIL') for r in d['data']['result']]"
kill $(lsof -ti:9090)

# 7. tdarr-exporter metrics
kubectl-homelab -n arr-stack port-forward svc/tdarr-exporter 9090:9090 &
sleep 3
curl -s http://localhost:9090/metrics | grep "tdarr_" | grep -v "#" | wc -l
kill $(lsof -ti:9090)

# 8. qbittorrent-exporter metrics
kubectl-homelab -n arr-stack port-forward svc/qbittorrent-exporter 8000:8000 &
sleep 3
curl -s http://localhost:8000/metrics | grep "qbittorrent_" | grep -v "#" | wc -l
kill $(lsof -ti:8000)

# 9. No false positives from Phase 4.28 alerts
# Open Prometheus → Status → Rules, verify all Phase 4.28 rules show health: ok
# None should be in Pending or Firing state (unless a real issue exists)

# 10. Grafana dashboards in Homelab folder
# Open https://grafana.k8s.rommelporras.com → Dashboards → Homelab folder
# Expected: 11 dashboards
```

---

## Rollback

```bash
# Remove Phase 4.28 alert rules
kubectl-homelab delete prometheusrule -n monitoring \
  uptime-kuma-alerts ghost-alerts invoicetron-alerts portfolio-alerts \
  storage-alerts cert-alerts cloudflare-alerts apiserver-alerts

# Restore arr-alerts (removes SeerrDown/TdarrDown/ByparrDown/ArrQueueWarning/ArrQueueError)
git checkout v0.26.0 -- manifests/monitoring/alerts/arr-alerts.yaml
kubectl-homelab apply -f manifests/monitoring/alerts/arr-alerts.yaml

# Remove Phase 4.28 probes
kubectl-homelab delete probe -n monitoring \
  jellyfin ghost invoicetron portfolio seerr tdarr byparr

# Remove Phase 4.28 ServiceMonitors
kubectl-homelab delete servicemonitor -n longhorn-system longhorn
kubectl-homelab delete servicemonitor -n cert-manager cert-manager
kubectl-homelab delete servicemonitor -n arr-stack tdarr-exporter qbittorrent-exporter

# Remove exporters
kubectl-homelab delete deployment,service -n arr-stack -l app=tdarr-exporter
kubectl-homelab delete deployment,service -n arr-stack -l app=qbittorrent-exporter
kubectl-homelab delete secret qbittorrent-exporter-secret -n arr-stack

# Uninstall smartctl-exporter
helm-homelab uninstall smartctl-exporter -n monitoring

# Remove ARR stall resolver
kubectl-homelab delete cronjob -n arr-stack arr-stall-resolver
kubectl-homelab delete configmap -n arr-stack arr-stall-resolver-script

# Remove Prometheus + Alertmanager HTTPRoutes
kubectl-homelab delete -f manifests/monitoring/grafana/prometheus-httproute.yaml
kubectl-homelab delete -f manifests/monitoring/grafana/alertmanager-httproute.yaml

# Revert Homepage widgets
git checkout v0.26.0 -- manifests/home/homepage/
kubectl-homelab apply -k manifests/home/homepage/

# Revert kubeadm bind addresses (back to 127.0.0.1)
for node in cp1.k8s.rommelporras.com cp2.k8s.rommelporras.com cp3.k8s.rommelporras.com; do
  ssh wawashi@$node 'sudo sed -i "s/--bind-address=0.0.0.0/--bind-address=127.0.0.1/" /etc/kubernetes/manifests/kube-scheduler.yaml'
  ssh wawashi@$node 'sudo sed -i "s/--bind-address=0.0.0.0/--bind-address=127.0.0.1/" /etc/kubernetes/manifests/kube-controller-manager.yaml'
  ssh wawashi@$node 'sudo sed -i "s|listen-metrics-urls=http://0.0.0.0:2381|listen-metrics-urls=http://127.0.0.1:2381|" /etc/kubernetes/manifests/etcd.yaml'
done

# Restore prior dashboard versions (roll back to v0.26.0)
git checkout v0.26.0 -- manifests/monitoring/dashboards/
kubectl-homelab apply -f manifests/monitoring/dashboards/

# Revert Grafana Helm values (removes folderAnnotation)
helm-homelab upgrade prometheus \
  oci://ghcr.io/prometheus-community/charts/kube-prometheus-stack \
  --version 81.0.0 \
  -n monitoring \
  -f <prior-values.yaml> \
  --set grafana.adminPassword="$(op read 'op://Kubernetes/Grafana/password')"
```
