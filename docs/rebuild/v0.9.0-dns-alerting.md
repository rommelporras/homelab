# v0.9.0 — AdGuard DNS Alerting

> **Release:** v0.9.0
> **Phase:** 4.8.1
> **Goal:** Alert when AdGuard DNS becomes unreachable (synthetic DNS monitoring)
> **Prerequisite:** [v0.8.0-gitlab.md](v0.8.0-gitlab.md) completed

---

## Overview

This release adds synthetic DNS monitoring to detect when AdGuard is running but not receiving traffic. This catches L2 lease misalignment where Cilium's L2 announcement points to the wrong node while `externalTrafficPolicy: Local` drops traffic silently.

**Components:**
- **Blackbox Exporter** — Separate Helm chart for synthetic probing (NOT bundled in kube-prometheus-stack)
- **Probe CRD** — Tells Prometheus to scrape blackbox exporter with DNS target
- **PrometheusRule** — Fires `AdGuardDNSUnreachable` alert after 2 minutes of failure

**Architecture:**
```
Prometheus → Blackbox Exporter → DNS query to 10.10.30.53 → AdGuard
                                         │
                                         ├─ Success: probe_success=1
                                         └─ Failure: probe_success=0 → Alert → Discord + Email
```

---

## Step 1: Enable Probe Discovery in Prometheus

Add to `helm/prometheus/values.yaml` under `prometheus.prometheusSpec`:

```yaml
probeSelectorNilUsesHelmValues: false  # Required for Probe CRDs (blackbox exporter)
```

Without this, Prometheus silently ignores all Probe CRDs.

Upgrade the stack:

```bash
./scripts/upgrade-prometheus.sh
```

---

## Step 2: Install Blackbox Exporter

**Important:** kube-prometheus-stack does NOT bundle blackbox exporter. It must be installed as a separate Helm chart.

### 2.1 Create Values File

**helm/blackbox-exporter/values.yaml:**

```yaml
# prometheus-blackbox-exporter Helm Values
# Custom probe modules - blackbox has NO default DNS module
config:
  modules:
    dns_udp:
      prober: dns
      timeout: 5s
      dns:
        transport_protocol: udp
        preferred_ip_protocol: ip4
        query_name: "google.com"
        query_type: "A"
        valid_rcodes:
          - NOERROR
    http_2xx:
      prober: http
      timeout: 5s
      http:
        preferred_ip_protocol: ip4
        valid_status_codes: []
        follow_redirects: true

resources:
  requests:
    cpu: 10m
    memory: 32Mi
  limits:
    memory: 64Mi

serviceMonitor:
  enabled: true
  defaults:
    labels:
      release: prometheus
```

### 2.2 Install

```bash
helm-homelab repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm-homelab repo update

helm-homelab install blackbox-exporter prometheus-community/prometheus-blackbox-exporter \
  --namespace monitoring \
  --version 11.7.0 \
  --values helm/blackbox-exporter/values.yaml
```

### 2.3 Verify

```bash
# Pod running
kubectl-homelab get pods -n monitoring | grep blackbox
# Expected: blackbox-exporter-prometheus-blackbox-exporter-xxxxx   1/1   Running

# Service name (needed for Probe CRD)
kubectl-homelab get svc -n monitoring | grep blackbox
# Expected: blackbox-exporter-prometheus-blackbox-exporter   ClusterIP
```

---

## Step 3: Create DNS Probe

The Probe CRD tells Prometheus to scrape blackbox exporter with a specific target and module.

**manifests/monitoring/adguard-dns-probe.yaml:**

```yaml
# AdGuard DNS synthetic monitoring
# Probes the LoadBalancer IP to detect L2 lease misalignment
apiVersion: monitoring.coreos.com/v1
kind: Probe
metadata:
  name: adguard-dns
  namespace: monitoring
  labels:
    app: adguard-dns-probe
spec:
  jobName: adguard-dns
  interval: 30s
  module: dns_udp
  prober:
    url: blackbox-exporter-prometheus-blackbox-exporter.monitoring.svc:9115
  targets:
    staticConfig:
      static:
        - 10.10.30.53
      labels:
        target_name: adguard-dns
```

```bash
kubectl-homelab apply -f manifests/monitoring/adguard-dns-probe.yaml
```

### Verify Probe

```bash
# Check Probe resource
kubectl-homelab get probe -n monitoring adguard-dns

# Port-forward to Prometheus
kubectl-homelab port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090 &

# Verify probe_success metric
curl -s "http://localhost:9090/api/v1/query?query=probe_success" | \
  jq '.data.result[] | select(.metric.job=="adguard-dns")'
# Expected: "value": [..., "1"]
```

---

## Step 4: Create Alert Rule

**manifests/monitoring/adguard-dns-alert.yaml:**

```yaml
# Alert when AdGuard DNS probe fails
# Catches L2 lease misalignment that pod health checks miss
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: adguard-dns-alerts
  namespace: monitoring
  labels:
    prometheus: prometheus
    role: alert-rules
spec:
  groups:
  - name: adguard-dns
    rules:
    - alert: AdGuardDNSUnreachable
      expr: probe_success{job="adguard-dns"} == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "AdGuard DNS is unreachable"
        description: "External DNS probe to 10.10.30.53 has failed for 2+ minutes. Likely L2 lease on wrong node."
        runbook: |
          1. Check pod node:
             kubectl-homelab get pods -n home -l app=adguard-home -o wide

          2. Check L2 lease holder:
             kubectl-homelab get leases -n kube-system cilium-l2announce-home-adguard-dns -o jsonpath='{.spec.holderIdentity}'

          3. If pod node != lease holder, delete lease to force re-election:
             kubectl-homelab delete lease -n kube-system cilium-l2announce-home-adguard-dns

          4. Verify DNS resolution restored:
             dig @10.10.30.53 google.com
```

```bash
kubectl-homelab apply -f manifests/monitoring/adguard-dns-alert.yaml
```

### Verify Alert Rule

```bash
# Check PrometheusRule
kubectl-homelab get prometheusrule -n monitoring adguard-dns-alerts

# Verify in Prometheus UI: http://localhost:9090/rules
# Look for "adguard-dns" group with AdGuardDNSUnreachable alert
```

---

## Step 5: Test Alert Pipeline (Optional)

Create a temporary probe targeting a non-existent IP to trigger the alert:

```bash
# Create test probe targeting non-existent DNS
cat <<'EOF' | kubectl-homelab apply -f -
apiVersion: monitoring.coreos.com/v1
kind: Probe
metadata:
  name: test-dns-fail
  namespace: monitoring
  labels:
    app: test-dns-probe
spec:
  jobName: test-dns-fail
  interval: 15s
  module: dns_udp
  prober:
    url: blackbox-exporter-prometheus-blackbox-exporter.monitoring.svc:9115
  targets:
    staticConfig:
      static:
        - 10.10.30.99
      labels:
        target_name: test-dns-fail
EOF

cat <<'EOF' | kubectl-homelab apply -f -
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: test-dns-alert
  namespace: monitoring
  labels:
    prometheus: prometheus
    role: alert-rules
spec:
  groups:
  - name: test-dns
    rules:
    - alert: TestDNSUnreachable
      expr: probe_success{job="test-dns-fail"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "TEST - DNS probe failure (safe to ignore)"
        description: "Test alert to verify DNS alerting pipeline."
EOF

# Wait ~3 minutes for alert to fire, then check Discord

# Clean up
kubectl-homelab delete probe -n monitoring test-dns-fail
kubectl-homelab delete prometheusrule -n monitoring test-dns-alert
```

---

## Verification Checklist

- [ ] Blackbox exporter pod running: `kubectl-homelab get pods -n monitoring | grep blackbox`
- [ ] dns_udp module configured: `kubectl-homelab get cm -n monitoring blackbox-exporter-prometheus-blackbox-exporter -o yaml | grep dns_udp`
- [ ] Probe resource exists: `kubectl-homelab get probe -n monitoring adguard-dns`
- [ ] Probe appears in Prometheus targets: http://localhost:9090/targets
- [ ] `probe_success{job="adguard-dns"}` returns value `1`
- [ ] PrometheusRule loaded: http://localhost:9090/rules (adguard-dns group)
- [ ] Test alert fires and Discord notification received (optional)

---

## Troubleshooting

### Probe not appearing in Prometheus targets

1. Verify `probeSelectorNilUsesHelmValues: false` is set in `helm/prometheus/values.yaml`
2. Run `./scripts/upgrade-prometheus.sh` if setting was just added
3. Check Probe resource exists: `kubectl-homelab get probe -A`

### probe_success always 0

1. Test blackbox exporter directly:
   ```bash
   kubectl-homelab exec -n monitoring deployment/blackbox-exporter-prometheus-blackbox-exporter -- \
     wget -qO- "http://localhost:9115/probe?target=10.10.30.53&module=dns_udp" | grep probe_success
   ```
2. Verify dns_udp module has `query_name` field (required, not default)
3. Verify AdGuard is responding: `dig @10.10.30.53 google.com`

### Alert not firing

1. Query Prometheus: `probe_success{job="adguard-dns"}`
2. Check PrometheusRule labels match Prometheus selector (`prometheus: prometheus`)
3. Check Alertmanager: `kubectl-homelab port-forward -n monitoring svc/prometheus-kube-prometheus-alertmanager 9093:9093`

---

## Files Reference

| File | Purpose |
|------|---------|
| `helm/prometheus/values.yaml` | Added `probeSelectorNilUsesHelmValues: false` |
| `helm/blackbox-exporter/values.yaml` | Blackbox exporter config with dns_udp module |
| `manifests/monitoring/adguard-dns-probe.yaml` | Probe CRD targeting AdGuard LB IP |
| `manifests/monitoring/adguard-dns-alert.yaml` | PrometheusRule for DNS failure alert |

---

## Key Learnings

| Topic | Lesson |
|-------|--------|
| Blackbox Exporter | NOT bundled in kube-prometheus-stack despite config key existing |
| dns_udp module | Not a default module — must be explicitly configured with `query_name` |
| Probe CRD | Requires `probeSelectorNilUsesHelmValues: false` or Prometheus ignores it |
| Service naming | Helm release name + chart name: `blackbox-exporter-prometheus-blackbox-exporter` |
| Job label | Comes from Probe's `jobName` field, not the full Prometheus target path |

---

## Next Steps

- Deploy additional services (Invoicetron, Uptime Kuma, Immich)
- Add HTTP probes for web services using the `http_2xx` module
