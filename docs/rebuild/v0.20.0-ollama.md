# v0.20.0 — Ollama Local AI

> **Release:** v0.20.0
> **Phase:** 4.23 (Ollama Local AI)
> **Goal:** Deploy CPU-only LLM inference server for Karakeep AI tagging (Phase 4.24)
> **Prerequisite:** [v0.3.0-storage.md](v0.3.0-storage.md) completed (Longhorn running)

---

## Overview

Deploys Ollama 0.15.6 with three models for AI-powered bookmark tagging:
- **qwen3:1.7b** — Primary text model (classification/tagging)
- **moondream** — Vision model (image description)
- **gemma3:1b** — Fallback text model

All inference runs on CPU (Intel i5-10400T). No GPU required.

Includes Blackbox HTTP probe monitoring and PrometheusRule alerts. CiliumNetworkPolicy restricts ingress to monitoring and karakeep namespaces only.

---

## Step 1: Apply Manifests

```bash
# Create namespace, deployment, service, and network policy
kubectl-homelab apply \
  -f manifests/ai/namespace.yaml \
  -f manifests/ai/ollama-deployment.yaml \
  -f manifests/ai/ollama-service.yaml \
  -f manifests/ai/networkpolicy.yaml

# Expected warning (PSS restricted warn — Ollama runs as root):
# Warning: would violate PodSecurity "restricted:latest": runAsNonRoot != true
```

## Step 2: Wait for Pod Ready

```bash
kubectl-homelab -n ai wait --for=condition=Ready pod -l app=ollama --timeout=180s
```

First startup pulls the 3.3GB Ollama image — may take a few minutes.

## Step 3: Pull Models

```bash
# Primary text model (~1.4 GB)
kubectl-homelab exec -n ai deploy/ollama -- ollama pull qwen3:1.7b

# Vision model (~1.7 GB)
kubectl-homelab exec -n ai deploy/ollama -- ollama pull moondream

# Fallback text model (~0.8 GB)
kubectl-homelab exec -n ai deploy/ollama -- ollama pull gemma3:1b
```

## Step 4: Verify Models

```bash
kubectl-homelab exec -n ai deploy/ollama -- ollama list
# Expected:
# gemma3:1b       815 MB
# moondream       1.7 GB
# qwen3:1.7b      1.4 GB
```

## Step 5: Test Inference

```bash
# Text inference
kubectl-homelab exec -n ai deploy/ollama -- ollama run qwen3:1.7b \
  "/nothink Classify this text into 3 tags: Kubernetes is a container orchestration platform"

# Cross-namespace service access
kubectl-homelab run curl-test --rm -it --image=curlimages/curl --restart=Never -- \
  curl -s http://ollama.ai.svc.cluster.local:11434
# Expected: "Ollama is running"
```

## Step 6: Apply Monitoring

```bash
kubectl-homelab apply \
  -f manifests/monitoring/ollama-probe.yaml \
  -f manifests/monitoring/ollama-alerts.yaml
```

## Step 7: Verify Monitoring

```bash
# Verify Blackbox probe reaches Ollama
kubectl-homelab run probe-check --rm -it --image=curlimages/curl --restart=Never -- \
  curl -s "http://blackbox-exporter-prometheus-blackbox-exporter.monitoring.svc:9115/probe?target=http://ollama.ai.svc.cluster.local:11434&module=http_2xx" \
  | grep probe_success
# Expected: probe_success 1
```

## Step 8: Verify Network Policy

```bash
# Should succeed (monitoring → ai allowed)
kubectl-homelab run allowed-test --rm -it --image=curlimages/curl --restart=Never \
  --overrides='{"spec":{"nodeName":"k8s-cp1"}}' -- \
  curl -s --max-time 5 "http://blackbox-exporter-prometheus-blackbox-exporter.monitoring.svc:9115/probe?target=http://ollama.ai.svc.cluster.local:11434&module=http_2xx" \
  | grep probe_success

# Should timeout (default namespace → ai blocked)
kubectl-homelab run blocked-test --rm -it --image=curlimages/curl --restart=Never -- \
  curl -s --max-time 5 http://ollama.ai.svc.cluster.local:11434
# Expected: exit code 28 (connection timed out)
```

---

## Verification Checklist

- [ ] Ollama pod `1/1 Running` in `ai` namespace
- [ ] All 3 models listed (`ollama list`)
- [ ] Text inference produces reasonable tags
- [ ] Service accessible from monitoring namespace
- [ ] Service blocked from default namespace
- [ ] Blackbox probe shows `probe_success 1`
- [ ] Node memory stays below 50% during inference

---

## Rollback

```bash
kubectl-homelab delete namespace ai
kubectl-homelab delete probe ollama -n monitoring
kubectl-homelab delete prometheusrule ollama-alerts -n monitoring
```

---

## Files

| File | Purpose |
|------|---------|
| manifests/ai/namespace.yaml | ai namespace (PSS baseline) |
| manifests/ai/ollama-deployment.yaml | Deployment + 10Gi PVC |
| manifests/ai/ollama-service.yaml | ClusterIP service (port 11434) |
| manifests/ai/networkpolicy.yaml | CiliumNetworkPolicy (ingress) |
| manifests/monitoring/ollama-probe.yaml | Blackbox HTTP probe |
| manifests/monitoring/ollama-alerts.yaml | PrometheusRule (3 alerts) |
