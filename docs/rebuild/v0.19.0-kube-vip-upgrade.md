# v0.19.0 — kube-vip Upgrade + Monitoring

> **Release:** v0.19.0
> **Phase:** 2.1 (kube-vip Upgrade + Monitoring)
> **Goal:** Upgrade kube-vip v1.0.3 → v1.0.4 (fix leader election errors) + add Prometheus monitoring
> **Prerequisite:** [v0.2.0-bootstrap.md](v0.2.0-bootstrap.md) completed (kube-vip running as static pods)

---

## Overview

Upgrades kube-vip on all 3 control plane nodes to fix stalled leader election errors (PRs #1383, #1386), then adds Prometheus monitoring with ServiceMonitor, alerting rules, and Grafana dashboard.

- **kube-vip v1.0.4:** Fixes `Failed to update lock optimistically` errors on leader node
- **Monitoring pattern:** Headless Service + manual Endpoints + ServiceMonitor (static pods have no selector)
- **Leader tracking:** kube-state-metrics lease metrics (kube-vip has no custom Prometheus metrics)
- **Alerts:** 4 rules covering instance down, all down, stale lease, and high restarts
- **Dashboard:** VIP status, instance health, process metrics, network I/O

---

## Step 1: Pre-pull Image on All Nodes

```bash
for node in cp1 cp2 cp3; do
  ssh wawashi@${node}.k8s.rommelporras.com \
    "sudo ctr image pull ghcr.io/kube-vip/kube-vip:v1.0.4"
done
```

---

## Step 2: Backup Current Manifests

```bash
for node in cp1 cp2 cp3; do
  ssh wawashi@${node}.k8s.rommelporras.com \
    "sudo cp /etc/kubernetes/manifests/kube-vip.yaml /etc/kubernetes/kube-vip.yaml.v1.0.3.bak"
done
```

---

## Step 3: Rolling Upgrade (Non-Leaders First, Leader Last)

Identify the current leader:
```bash
kubectl-homelab get lease plndr-cp-lock -n kube-system -o jsonpath='{.spec.holderIdentity}'
```

Upgrade non-leader nodes first, then the leader last:
```bash
# For each node (non-leaders first):
ssh wawashi@cpX.k8s.rommelporras.com \
  "sudo sed -i 's|ghcr.io/kube-vip/kube-vip:v1.0.3|ghcr.io/kube-vip/kube-vip:v1.0.4|' /etc/kubernetes/manifests/kube-vip.yaml"

# Wait 30s between each node, verify pod is Running:
kubectl-homelab get pods -n kube-system -l component=kube-vip -o wide
```

After upgrading the leader node, verify VIP failover:
```bash
curl -sk https://10.10.30.10:6443/healthz
# Expected: ok
```

---

## Step 4: Verify Upgrade

```bash
# All 3 pods running v1.0.4
kubectl-homelab get pods -n kube-system -l component=kube-vip -o wide

# No more lease errors
for node in k8s-cp1 k8s-cp2 k8s-cp3; do
  echo "--- $node ---"
  kubectl-homelab logs -n kube-system kube-vip-${node} --tail=5
done

# New leader elected
kubectl-homelab get lease plndr-cp-lock -n kube-system -o jsonpath='{.spec.holderIdentity}'
```

---

## Step 5: Apply Monitoring Manifests

```bash
# ServiceMonitor + Headless Service + Endpoints
kubectl-homelab apply -f manifests/monitoring/kube-vip-monitoring.yaml

# PrometheusRule (4 alerts)
kubectl-homelab apply -f manifests/monitoring/kube-vip-alerts.yaml

# Grafana dashboard
kubectl-homelab apply -f manifests/monitoring/kube-vip-dashboard-configmap.yaml
```

---

## Step 6: Verify Monitoring

```bash
# Check Prometheus targets (all 3 should be UP)
kubectl-homelab exec -n monitoring prometheus-prometheus-kube-prometheus-prometheus-0 -- \
  wget -qO- 'http://localhost:9090/api/v1/targets?scrapePool=serviceMonitor/monitoring/kube-vip/0' 2>/dev/null | \
  python3 -c "import json,sys; data=json.load(sys.stdin); [print(f\"{t['labels']['instance']}: {t['health']}\") for t in data['data']['activeTargets']]"

# Check alerts loaded (4 rules, all inactive = healthy)
kubectl-homelab exec -n monitoring prometheus-prometheus-kube-prometheus-prometheus-0 -- \
  wget -qO- 'http://localhost:9090/api/v1/rules?type=alert' 2>/dev/null | \
  python3 -c "import json,sys; data=json.load(sys.stdin); [print(f\"{r['name']}: {r['state']}\") for g in data['data']['groups'] if g['name']=='kube-vip' for r in g['rules']]"
```

Dashboard: Navigate to Grafana → Dashboards → "kube-vip VIP Health"

---

## Rollback

```bash
# Revert a single node to v1.0.3
ssh wawashi@cpX.k8s.rommelporras.com \
  "sudo cp /etc/kubernetes/kube-vip.yaml.v1.0.3.bak /etc/kubernetes/manifests/kube-vip.yaml"

# Revert all nodes
for node in cp1 cp2 cp3; do
  ssh wawashi@${node}.k8s.rommelporras.com \
    "sudo cp /etc/kubernetes/kube-vip.yaml.v1.0.3.bak /etc/kubernetes/manifests/kube-vip.yaml"
done

# Remove monitoring
kubectl-homelab delete -f manifests/monitoring/kube-vip-monitoring.yaml
kubectl-homelab delete -f manifests/monitoring/kube-vip-alerts.yaml
kubectl-homelab delete -f manifests/monitoring/kube-vip-dashboard-configmap.yaml
```

---

## Files

| File | Purpose |
|------|---------|
| manifests/monitoring/kube-vip-monitoring.yaml | Headless Service + Endpoints + ServiceMonitor |
| manifests/monitoring/kube-vip-alerts.yaml | PrometheusRule with 4 alerts |
| manifests/monitoring/kube-vip-dashboard-configmap.yaml | Grafana dashboard ConfigMap |
| ansible/group_vars/all.yml | kubevip_version: v1.0.3 → v1.0.4 |
