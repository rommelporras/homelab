# v0.15.0 — Claude Code Monitoring

> **Release:** v0.15.0
> **Phase:** 4.15
> **Goal:** Centralize Claude Code telemetry on homelab K8s cluster
> **Prerequisite:** [v0.4.0 rebuild](v0.4.0-monitoring.md) completed (kube-prometheus-stack + Loki running)

---

## Overview

This release deploys an OpenTelemetry Collector to receive Claude Code metrics and events via OTLP, exporting metrics to Prometheus and events to Loki. A Grafana dashboard and cost alerts are auto-provisioned via ConfigMaps and PrometheusRules.

**Architecture:**
```
Client Machines (TRUSTED_WIFI / LAN)
┌───────────────────────────────────┐
│ Claude Code                       │
│ OTLP gRPC → 10.10.30.22:4317     │
└───────────────┬───────────────────┘
                │
                ▼
K8s Cluster (monitoring namespace)
┌───────────────────────────────────┐
│ OTel Collector (Deployment)       │
│ - OTLP gRPC :4317, HTTP :4318    │
│ - Prometheus exporter :8889       │
│ - Loki push (otlphttp/loki)      │
│ - LoadBalancer: 10.10.30.22      │
└────────┬──────────────┬───────────┘
         │              │
         ▼              ▼
   Prometheus         Loki
   (metrics)        (events)
         │              │
         └──────┬───────┘
                ▼
            Grafana
   Claude Code Dashboard
```

**Components:**

| Component | Version | Purpose |
|-----------|---------|---------|
| OTel Collector (contrib) | v0.144.0 | OTLP receiver, metric/log exporter |
| Grafana Dashboard | ConfigMap | 33 panels across 8 sections |
| PrometheusRule | claude-code-alerts | 4 alert rules (cost, availability) |

---

## Step 1: Apply OTel Collector Manifests

```bash
kubectl-homelab apply -f manifests/monitoring/otel-collector-config.yaml
kubectl-homelab apply -f manifests/monitoring/otel-collector.yaml
kubectl-homelab apply -f manifests/monitoring/otel-collector-servicemonitor.yaml
```

This deploys:
- **otel-collector-config.yaml** — ConfigMap with pipeline config (OTLP receivers → memory_limiter + batch processors → Prometheus + Loki exporters)
- **otel-collector.yaml** — Deployment (1 replica, v0.144.0) + LoadBalancer Service (VIP 10.10.30.22)
- **otel-collector-servicemonitor.yaml** — ServiceMonitor for Prometheus to scrape OTel Collector metrics

Verify:
```bash
# Pod running
kubectl-homelab get pods -n monitoring -l app=otel-collector

# Health check responding
kubectl-homelab exec -n monitoring deploy/otel-collector -- wget -qO- http://localhost:13133/

# LoadBalancer VIP assigned
kubectl-homelab get svc otel-collector -n monitoring
# EXTERNAL-IP should show 10.10.30.22

# VIP reachable from client machine
curl -s http://10.10.30.22:13133/
```

---

## Step 2: Verify Loki OTLP Ingestion

Loki v3.0+ enables OTLP ingestion by default (`allow_structured_metadata: true`). Our Loki is v3.6.3, so this should work out of the box.

Verify:
```bash
kubectl-homelab -n monitoring exec deploy/loki -- cat /etc/loki/local-config.yaml | grep -A2 structured_metadata
```

If not enabled, add to `helm/loki/values.yaml`:
```yaml
loki:
  limits_config:
    allow_structured_metadata: true
```

Then upgrade:
```bash
./scripts/upgrade-loki.sh  # or helm-homelab upgrade
```

---

## Step 3: Import Dashboard and Alerts

```bash
kubectl-homelab apply -f manifests/monitoring/claude-dashboard-configmap.yaml
kubectl-homelab apply -f manifests/monitoring/claude-alerts.yaml
```

This deploys:
- **claude-dashboard-configmap.yaml** — Grafana dashboard with `grafana_dashboard: "1"` label (auto-provisioned by Grafana sidecar)
- **claude-alerts.yaml** — PrometheusRule with 4 alerts:
  - `ClaudeCodeHighDailySpend` — warning when >$25/day
  - `ClaudeCodeCriticalDailySpend` — critical when >$50/day
  - `ClaudeCodeNoActivity` — info when no activity detected at end of weekday (5-6pm)
  - `OTelCollectorDown` — critical when collector unreachable

Verify dashboard:
1. Open https://grafana.k8s.rommelporras.com
2. Search for "Claude Code Monitoring" dashboard
3. Dashboard should appear (panels will be empty until client sends data)

Verify alerts:
```bash
# Check PrometheusRule is loaded
kubectl-homelab get prometheusrules -n monitoring | grep claude
```

---

## Step 4: Configure Client Machines

Add to `~/.zshrc` on each machine that runs Claude Code:

```bash
# Claude Code Telemetry → Homelab OTel Collector
export CLAUDE_CODE_ENABLE_TELEMETRY=1
export OTEL_METRICS_EXPORTER=otlp
export OTEL_LOGS_EXPORTER=otlp
export OTEL_EXPORTER_OTLP_PROTOCOL=grpc
export OTEL_EXPORTER_OTLP_ENDPOINT=http://10.10.30.22:4317
export OTEL_METRIC_EXPORT_INTERVAL=60000
export OTEL_LOGS_EXPORT_INTERVAL=5000
export OTEL_RESOURCE_ATTRIBUTES="machine.name=$HOST"
```

`$HOST` resolves to the machine hostname automatically.

Apply:
```bash
source ~/.zshrc
```

---

## Step 5: Validate End-to-End

Start a Claude Code session and verify data flows through the pipeline.

### Metrics (Prometheus)

```bash
# Check ServiceMonitor target is UP
# Port-forward Prometheus or check via Grafana Explore

# Query for Claude Code metrics
curl -s "http://localhost:9090/api/v1/label/__name__/values" | jq '.data[] | select(startswith("claude_code"))'
```

Expected metrics:
- `claude_code_cost_usage_USD_total`
- `claude_code_token_usage_tokens_total`
- `claude_code_session_count_total`
- `claude_code_active_time_seconds_total`

### Events (Loki)

In Grafana Explore (Loki datasource):
```logql
{service_name="claude-code"}
```

Expected event types (structured metadata `event_name`):
- `api_request`
- `tool_result`
- `tool_decision`
- `user_prompt`

### Dashboard

Open the "Claude Code Monitoring" dashboard in Grafana. After a few minutes of Claude Code usage, all 8 sections should populate:
- Overview / Trends: cost, session stats, timeseries graphs
- Productivity / Sessions & Activity: CLI time, tool calls, prompts per hour
- Cost Analysis / Token & Efficiency: spend breakdowns, token pie charts
- Performance (Events): Loki-based API latency, tool usage
- Insights: CLI vs User time, tool usage distribution, tool activity, prompt rate

---

## Step 6: Retire Local Docker Compose Stack

After verifying the homelab pipeline works for 24+ hours:

```bash
cd ~/personal/claude-monitoring
docker compose down
docker volume rm claude_prometheus_data claude_grafana_data
```

Keep the `~/personal/claude-monitoring` repo for the open-source project.

---

## Verification Checklist

- [ ] OTel Collector pod running in monitoring namespace
- [ ] LoadBalancer VIP 10.10.30.22 assigned and reachable
- [ ] Health check endpoint responding on :13133
- [ ] Prometheus scraping OTel Collector (ServiceMonitor target UP)
- [ ] Loki OTLP ingestion enabled (allow_structured_metadata)
- [ ] `claude_code_cost_usage_USD_total` metric visible in Prometheus
- [ ] Events visible in Loki: `{service_name="claude-code"}`
- [ ] Grafana dashboard loaded with 33 panels across 8 sections
- [ ] Cost alerts registered in Alertmanager
- [ ] Client machine sending metrics (source ~/.zshrc, run Claude Code)
- [ ] Local Docker Compose stack stopped

---

## Rollback

```bash
# Remove all Claude monitoring resources
kubectl-homelab delete -f manifests/monitoring/claude-alerts.yaml
kubectl-homelab delete -f manifests/monitoring/claude-dashboard-configmap.yaml
kubectl-homelab delete -f manifests/monitoring/otel-collector-servicemonitor.yaml
kubectl-homelab delete -f manifests/monitoring/otel-collector.yaml
kubectl-homelab delete -f manifests/monitoring/otel-collector-config.yaml

# Re-enable local Docker Compose stack if needed
cd ~/personal/claude-monitoring && docker compose up -d

# Revert client env vars in ~/.zshrc
# (set OTEL_EXPORTER_OTLP_ENDPOINT back to http://localhost:4317)
```

---

## Files Reference

| File | Purpose |
|------|---------|
| manifests/monitoring/otel-collector-config.yaml | OTel Collector pipeline config (ConfigMap) |
| manifests/monitoring/otel-collector.yaml | Deployment + LoadBalancer Service |
| manifests/monitoring/otel-collector-servicemonitor.yaml | ServiceMonitor for Prometheus scraping |
| manifests/monitoring/claude-dashboard-configmap.yaml | Grafana dashboard (33 panels, 8 sections) |
| manifests/monitoring/claude-alerts.yaml | PrometheusRule (4 cost/availability alerts) |

---

## Key Learnings

| Topic | Lesson |
|-------|--------|
| OTel Collector memory | Container memory limit (600Mi) must exceed memory_limiter (512 MiB) or pod OOM-kills |
| Loki OTLP ingestion | Use `otlphttp/loki` exporter with endpoint `/otlp`, not deprecated `loki` exporter |
| health_check extension | Must explicitly enable health_check extension for liveness/readiness probes on :13133 |
| Per-session counters | All Claude Code metrics are per-session; use `increase()` or `rate()`, never raw `sum()` |
| OTEL_RESOURCE_ATTRIBUTES | Use `machine.name=$HOST` to auto-identify which machine generated metrics |
| Prometheus metric names | OTLP dots become underscores, counters get `_total` suffix — verify after deployment |
| Loki event names | OTLP native ingestion stores attributes as structured metadata — query with `\| event_name="api_request"`, not `\| json` |
| Security hardening | OTel Collector supports runAsNonRoot (UID 10001), readOnlyRootFilesystem, drop ALL capabilities, seccompProfile RuntimeDefault |
