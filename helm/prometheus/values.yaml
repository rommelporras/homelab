# kube-prometheus-stack Helm Values for Homelab
# Docs: https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack
#
# Prerequisites:
#   kubectl-homelab create namespace monitoring
#   kubectl-homelab label namespace monitoring pod-security.kubernetes.io/enforce=privileged
#   (privileged required: node-exporter needs hostNetwork, hostPID, hostPath)
#
# Install via OCI (no helm repo add needed):
#   helm-homelab install prometheus oci://ghcr.io/prometheus-community/charts/kube-prometheus-stack \
#     --namespace monitoring \
#     --version 81.0.0 \
#     --values helm/prometheus/values.yaml \
#     --set grafana.adminPassword="$(op read 'op://Kubernetes/Grafana/password')"

# =============================================================================
# kube-proxy
# =============================================================================
# Disabled: Cilium runs in kube-proxy replacement mode ‚Äî no kube-proxy DaemonSet exists.
# Without this, KubeProxyDown fires permanently and 3 scrape targets show as down.
kubeProxy:
  enabled: false

# =============================================================================
# Default Rules
# =============================================================================
# Disable alerts that aren't useful for this homelab setup
defaultRules:
  disabled:
    # InfoInhibitor is a meta-alert for complex info-suppression logic
    # We don't use this - our routing is simple (info ‚Üí discord-status)
    InfoInhibitor: true
    # Watchdog is enabled - routes to healthchecks.io as dead man's switch
    # If alerting breaks, healthchecks.io detects missing pings and alerts via Discord
    #
    # CPUThrottlingHigh is NOT disabled globally - we want alerts for app workloads
    # Monitoring namespace is silenced via Alertmanager route (see routes section)

# =============================================================================
# Prometheus
# =============================================================================
# Time-series database that scrapes and stores all metrics
prometheus:
  prometheusSpec:
    # How long to keep data (default: 10d)
    retention: 90d

    # Storage - uses Longhorn PVC
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi

    # Resource limits for 16GB nodes
    # Prometheus can be memory-hungry, especially with many metrics
    resources:
      requests:
        cpu: 200m
        memory: 1Gi
      limits:
        cpu: 1000m
        memory: 2Gi

    # Scrape all ServiceMonitors in all namespaces (not just monitoring)
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false  # Required for Probe CRDs (blackbox exporter)

# =============================================================================
# Grafana
# =============================================================================
# Visualization dashboards
grafana:
  # Admin password set via --set at install time (from 1Password)
  # adminPassword: set at install time

  # Persistence - dashboards survive pod restarts
  persistence:
    enabled: true
    storageClassName: longhorn
    size: 10Gi

  # Resource limits
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

  # Grafana configuration
  grafana.ini:
    server:
      # Root URL for links in emails, etc.
      root_url: https://grafana.k8s.rommelporras.com

    # Disable analytics/telemetry
    analytics:
      reporting_enabled: false
      check_for_updates: false

# =============================================================================
# Alertmanager
# =============================================================================
# Handles alerts from Prometheus
#
# Notification Routing:
#   - Critical alerts ‚Üí Discord #incidents + Email (critical@rommelporras.com)
#   - Warning/Info alerts ‚Üí Discord #status only
#
# Secrets are injected at install/upgrade time via --set flags (from 1Password)
# See: scripts/upgrade-prometheus.sh
#
alertmanager:
  alertmanagerSpec:
    # Storage for silences and notifications
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 128Mi

  # Alertmanager configuration
  # Docs: https://prometheus.io/docs/alerting/latest/configuration/
  config:
    global:
      # SMTP settings for email notifications
      smtp_smarthost: 'smtp.mail.me.com:587'
      smtp_from: 'noreply@rommelporras.com'
      smtp_auth_username: 'SET_VIA_HELM'  # Injected from 1Password
      smtp_auth_password: 'SET_VIA_HELM'  # Injected from 1Password
      smtp_require_tls: true

      # Discord webhook URLs (set via --set flags)
      # These are referenced in receivers below

    # Default route - all alerts go here unless matched by sub-routes
    route:
      receiver: 'discord-status'
      group_by: ['alertname', 'namespace', 'job']
      group_wait: 30s        # Wait before sending first notification
      group_interval: 5m     # Wait between notifications for same group
      repeat_interval: 4h    # Re-notify if alert still firing

      # Sub-routes (order matters - first match wins)
      routes:
        # =================================================================
        # SILENCED: kubeadm control plane scraping issues
        # kubeadm binds components to 127.0.0.1, so Prometheus can't scrape
        # The components work fine. TODO: Fix in future (docs/todo/deferred.md)
        - match:
            alertname: KubeProxyDown
          receiver: 'null'
          continue: false
        - match:
            alertname: etcdInsufficientMembers
          receiver: 'null'
          continue: false
        - match:
            alertname: etcdMembersDown
          receiver: 'null'
          continue: false
        - match_re:
            alertname: TargetDown
            job: kube-scheduler|kube-controller-manager|kube-etcd
          receiver: 'null'
          continue: false

        # =================================================================
        # SILENCED: CPU throttling in monitoring namespace
        # node-exporter and kube-state-metrics burst during scrapes - expected behavior
        # We removed CPU limits from these, but silence the alert as backup
        # App workloads (adguard, grafana, etc.) still get throttling alerts
        - match:
            alertname: CPUThrottlingHigh
            namespace: monitoring
          receiver: 'null'
          continue: false

        # =================================================================
        # Watchdog ‚Üí healthchecks.io (dead man's switch)
        # =================================================================
        # Watchdog always fires. If it stops, healthchecks.io alerts us.
        - match:
            alertname: Watchdog
          receiver: 'healthchecks-heartbeat'
          repeat_interval: 1m
          continue: false

        # =================================================================
        # Active alert routing
        # =================================================================
        # Critical alerts ‚Üí #incidents + Email
        - match:
            severity: critical
          receiver: 'discord-incidents-email'
          continue: false

        # Warning alerts ‚Üí #status only
        - match:
            severity: warning
          receiver: 'discord-status'
          continue: false

        # Info alerts ‚Üí #status only (default catches these too)
        - match:
            severity: info
          receiver: 'discord-status'
          continue: false

    # Notification receivers
    receivers:
      # Critical: Discord #incidents + Email
      - name: 'discord-incidents-email'
        discord_configs:
          - webhook_url: 'SET_VIA_HELM'  # Injected from 1Password
            title: 'üî¥ {{ .Status | toUpper }}: {{ .CommonLabels.alertname }}'
            message: |
              {{ range .Alerts }}
              **{{ .Labels.alertname }}** ({{ .Labels.severity }})
              {{ .Annotations.summary }}
              {{ if .Annotations.description }}{{ .Annotations.description }}{{ end }}
              {{ end }}
        email_configs:
          # Multiple recipients for critical alerts (redundancy)
          - to: 'critical@rommelporras.com, r3mmel023@gmail.com, rommelcporras@gmail.com'
            send_resolved: true
            headers:
              Subject: '[{{ .Status | toUpper }}] {{ .CommonLabels.alertname }}'

      # Warning/Info: Discord #status only
      - name: 'discord-status'
        discord_configs:
          - webhook_url: 'SET_VIA_HELM'  # Injected from 1Password
            title: '{{ if eq .Status "firing" }}‚ö†Ô∏è{{ else }}‚úÖ{{ end }} {{ .Status | toUpper }}: {{ .CommonLabels.alertname }}'
            message: |
              {{ range .Alerts }}
              **{{ .Labels.alertname }}** ({{ .Labels.severity }})
              {{ .Annotations.summary }}
              {{ if .Annotations.description }}{{ .Annotations.description }}{{ end }}
              {{ end }}

      # Fallback null receiver (for silencing)
      - name: 'null'

      # Healthchecks.io heartbeat (dead man's switch)
      # Watchdog pings this URL every minute. If pings stop, healthchecks.io alerts.
      - name: 'healthchecks-heartbeat'
        webhook_configs:
          - url: 'SET_VIA_HELM'  # Injected from 1Password at runtime
            send_resolved: false

    # Inhibition rules - suppress lower severity when higher severity fires
    inhibit_rules:
      # If critical is firing, suppress warning for same alertname
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'namespace']

# =============================================================================
# node-exporter
# =============================================================================
# DaemonSet that runs on every node to collect hardware/OS metrics
# CKA Topic: DaemonSets ensure one pod per node
prometheus-node-exporter:
  resources:
    requests:
      cpu: 50m
      memory: 32Mi
    limits:
      # No CPU limit - node-exporter bursts during scrapes, throttling is counterproductive
      # CPU is compressible; memory is not. Only memory limits matter for stability.
      memory: 64Mi

# =============================================================================
# kube-state-metrics
# =============================================================================
# Exposes metrics about Kubernetes objects (pods, deployments, etc.)
kube-state-metrics:
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      # No CPU limit - same rationale as node-exporter
      memory: 128Mi

# =============================================================================
# Prometheus Operator
# =============================================================================
# Manages Prometheus and Alertmanager instances via CRDs
prometheusOperator:
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 300m
      memory: 256Mi
